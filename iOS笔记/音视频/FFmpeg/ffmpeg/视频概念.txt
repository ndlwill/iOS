视频封装格式，简称视频格式：
它里面包含了封装视频文件所需要的视频信息、音频信息和相关的配置信息，相当于一种储存视频信息的容器(比如：视频和音频的关联信息、如何解码等等)。
一种视频封装格式的直接反映就是对应着相应的视频文件格式。


===================================================================================================
视频编解码方式：
视频编解码的过程是指对数字视频进行压缩或解压缩的一个过程

常见的视频编码方式有：
H.264，等同于 MPEG-4 第十部分，也被称为高级视频编码(Advanced Video Coding，简称 AVC)，
是一种视频压缩标准，一种被广泛使用的高精度视频的录制、压缩和发布格式。

H.265，被称为高效率视频编码(High Efficiency Video Coding，简称 HEVC)是一种视频压缩标准，是 H.264 的继任者。
HEVC 被认为不仅提升图像质量，同时也能达到 H.264 两倍的压缩率（等同于同样画面质量下比特率减少了 50%），
可支持 4K 分辨率甚至到超高画质电视，最高分辨率可达到 8192×4320（8K 分辨率），这是目前发展的趋势。

「视频封装格式」看做是一个装着视频、音频、「视频编解码方式」等信息的容器。
一种「视频封装格式」可以支持多种「视频编解码方式」

看到一个视频文件名为 test.mov 时，我们可以知道它的「视频文件格式」是 .mov，也可以知道它的视频封装格式是 QuickTime File Format，
但是无法知道它的「视频编解码方式」。

那比较专业的说法可能是以 A/B 这种方式，A 是「视频编解码方式」，B 是「视频封装格式」。
比如：一个 H.264/MOV 的视频文件，它的封装方式就是 QuickTime File Format，编码方式是 H.264

===================================================================================================
音频编解码方式：
常见的音频编码方式有：
AAC，英文全称 Advanced Audio Coding，又称为 MPEG-4 AAC。
MP3，英文全称 MPEG-1 or MPEG-2 Audio Layer III，是当曾经非常流行的一种数字音频编码和有损压缩格式，它被设计来大幅降低音频数据量。
WMA，英文全称 Windows Media Audio，由微软公司开发的一种数字音频压缩格式，本身包括有损和无损压缩格式。

===================================================================================================
H.264:
关于 H.264 相关的概念，从大到小排序依次是：序列、图像、片组、片、NALU、宏块、亚宏块、块、像素

图像:
一帧通常就是一幅完整的图像。
当采集视频信号时，如果采用逐行扫描，则每次扫描得到的信号就是一副图像，也就是一帧。
当采集视频信号时，如果采用隔行扫描（奇、偶数行），则扫描下来的一帧图像就被分为了两个部分，这每一部分就称为「场」，根据次序分为：「顶场」和「底场」。
逐行扫描适合于运动图像，所以对于运动图像采用帧编码更好；隔行扫描适合于非运动图像，所以对于非运动图像采用场编码更好。

片(Slice):
每一帧图像可以分为多个片。   

网络提取层单元(NALU, Network Abstraction Layer Unit)，NALU 是用来将编码的数据进行打包的
一个分片(Slice)可以编码到一个 NALU 单元。
不过一个 NALU 单元中除了容纳分片(Slice)编码的码流外，还可以容纳其他数据，比如序列参数集 SPS。
对于客户端其主要任务则是接收数据包，从数据包中解析出 NALU 单元，然后进行解码播放

宏块(Macroblock):
分片是由宏块组成

颜色模型:
RGB 模型
在 YUV 中 Y 表示的是「亮度」，也就是灰阶值，U 和 V 则是表示「色度」。
YUV 的关键是在于它的亮度信号 Y 和色度信号 U、V 是分离的。那就是说即使只有 Y 信号分量而没有 U、V 分量，我们仍然可以表示出图像，只不过图像是黑白灰度图像

YCbCr 是属于 YUV 家族的一员,在YCbCr 中 Y 是指亮度分量，Cb 指蓝色色度分量，而 Cr 指红色色度分量。

图像和图像中的像素:
一副图片就是一个像素阵列

码流格式:
如何从 H.264 的码流中找到那些像素

NAL 包将其负载数据存储在 RBSP(Raw Byte Sequence Payload) 中，RBSP 是一系列的 SODB(String Of Data Bits)。


H.264 的码流结构 : bitstream_detailed.png
一张图片可以包含一个或多个分片(Slice)，而每一个分片(Slice)又会被分为了若干宏块(Macroblock)。
每个分片也包含着头和数据两部分,分片头中包含着分片类型、分片中的宏块类型、分片帧的数量以及对应的帧的设置和参数等信息，而分片数据中则是宏块，这里就是我们要找的存储像素数据的地方。
宏块是视频信息的主要承载者，因为它包含着每一个像素的亮度和色度信息,视频解码最主要的工作则是提供高效的方式从码流中获得宏块中的像素阵列。
宏块中包含了宏块类型、预测类型、Coded Block Pattern、Quantization Parameter、像素的亮度和色度数据集等等信息

===================================================================================================
帧内预测和帧间预测

宏块的数据结构，其中提到了「预测类型」这个字段
「帧内预测」和「帧间预测」这两种在视频编码中常用到的压缩方法，也可以称为「帧内压缩」和「帧间压缩」。
帧内压缩类似于图片压缩，跟这一帧的前面（或后面）一帧（或几帧）无关，由当前帧中，已编码的部分来推测当前待编码的这一部分数据是什么。
帧间压缩是由这一帧的前（或后）一帧（或几帧）来推测当前待压缩的这一部分数据是什么。

图片可以划分为宏块。一般来说，运动多细节多的部分，划分成小块来编码，无变化的大片的部分，划分成大块来编码。

对于帧间压缩，可以看下面连续的两帧,可以看到前后两帧的差异其实是很小的，这时候用帧间压缩就很有意义。
做帧间压缩常用的方式就是块匹配(Block Matching)，就是找找看前面已经编码的几帧里面，和我当前这个块最类似的一个块，这样我就不用编码当前块的内容了，只需要编码当前块和我找到的那个块的差异（称为残差）即可。
找最像的块的过程叫运动搜索（Motion Search），又叫运动估计（Motion Estimation）。用残差和原来的块就能推算出当前块的过程叫运动补偿（Motion Compensation）。

===================================================================================================
流媒体协议

RTP，实时传输协议，Real-time Transport Protocol，是一种网络传输协议，运行在 UDP 协议之上，RTP协议详细说明了在互联网上传递音频和视频的标准数据包格式。RTP协议常用于流媒体系统（配合 RTSP 协议）。
RTCP，实时传输控制协议，Real-time Transport Control Protocol，是实时传输协议（RTP）的一个姐妹协议。RTCP为RTP媒体流提供信道外（out-of-band）控制。RTCP 本身并不传输数据，但和 RTP 一起协作将多媒体数据打包和发送。RTCP 定期在流多媒体会话参加者之间传输控制数据。RTCP 的主要功能是为 RTP 所提供的服务质量（Quality of Service）提供反馈。
RTSP，实时流传输协议，Real Time Streaming Protocol，该协议定义了一对多应用程序如何有效地通过 IP 网络传送多媒体数据。RTSP 在体系结构上位于 RTP 和 RTCP 之上，它使用 TCP 或 UDP 完成数据传输。使用 RTSP 时，客户机和服务器都可以发出请求，即 RTSP 可以是双向的。
RTMP，实时消息传输协议，Real Time Messaging Protocol，是 Adobe Systems 公司为 Flash 播放器和服务器之间音频、视频和数据传输开发的开放协议。协议基于 TCP，是一个协议族，包括 RTMP 基本协议及 RTMPT/RTMPS/RTMPE 等多种变种。RTMP 是一种设计用来进行实时数据通信的网络协议，主要用来在 Flash/AIR 平台和支持RTMP协议的流媒体/交互服务器之间进行音视频和数据通信。
RTMFP，是 Adobe 公司开发的一套新的通信协议，全称 Real Time Media Flow Protocol，协议基于 UDP，支持 C/S 模式和 P2P 模式，即该协议可以让使用 Adobe Flash Player 的终端用户之间进行直接通信。
HTTP，超文本传输协议，HyperText Transfer Protocol，运行在 TCP 之上，这个协议是大家非常熟悉的，它也可以用到视频业务中来。
HLS，是苹果公司实现的基于 HTTP 的流媒体传输协议，全称 HTTP Live Streaming，可支持流媒体的直播和点播，主要应用在 iOS 系统，为 iOS 设备（如 iPhone、iPad）提供音视频直播和点播方案。对于 HLS 点播，基本上就是常见的分段 HTTP 点播，不同在于，它的分段非常小。要实现HLS点播，重点在于对媒体文件分段。对于 HLS 直播，相对于常见的流媒体直播协议，例如 RTMP 协议、RTSP 协议等，HLS 最大的不同在于直播客户端获取到的并不是一个完整的数据流，而是连续的、短时长的媒体文件（MPEG-TS 格式），客户端不断的下载并播放这些小文件。由于数据通过 HTTP 协议传输，所以完全不用考虑防火墙或者代理的问题，而且分段文件的时长很短，客户端可以很快的选择和切换码率，以适应不同带宽条件下的播放。不过 HLS 的这种技术特点，决定了它的延迟一般总是会高于普通的流媒体直播协议。


网络视频点播：
网络视频点播业务采用 HTTP 有两方面优势：
HTTP 是基于 TCP 协议的应用层协议，媒体传输过程中不会出现丢包等现象，从而保证了视频的质量。
HTTP 是绝大部分的 Web 服务器支持的协议，因而流媒体服务机构不必投资购买额外的流媒体服务器，从而节约了开支。

视频编码标准和音频编码标准是 H.264 和 AAC，这两种标准分别是当今实际应用中编码效率最高的视频标准和音频标准



网络视频直播：
网络视频直播服务采用 RTMP 作为直播协议的好处是可以直接被 Flash 播放器支持，而 Flash 播放器在 PC 时代有着极高的普及率，并且与浏览器结合的很好。因此这种流媒体直播平台基本上可以实现了「无插件直播」，极大降低了用户使用成本。
封装格式、视频编码、音频编码、播放器方面几乎全部采用了 FLV、H.264、AAC、Flash。FLV、RTMP、Flash 都是 Adobe 公司的产品，天生有着良好的结合性。