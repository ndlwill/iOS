Framework
Core ML
Integrate machine learning models into your app.

iOS 11.0

https://developer.apple.com/documentation/coreml

Use Core ML to integrate machine learning models into your app. 
Core ML provides a unified representation for all models. 
Your app uses Core ML APIs and user data to make predictions, and to train or fine-tune models, all on a person’s device.


###
A model is the result of applying a machine learning algorithm to a set of training data. 
###
You use a model to make predictions based on new input data. Models can accomplish a wide variety of tasks that would be difficult or impractical to write in code. 
For example, you can train a model to categorize photos, or detect specific objects within a photo directly from its pixels.


You build and train a model with the Create ML app bundled with Xcode. 
Models trained using Create ML are in the Core ML model format and are ready to use in your app.
###
Create ML
https://developer.apple.com/machine-learning/create-ml/
###
Alternatively, you can use a wide variety of other machine learning libraries and then use Core ML Tools to convert the model into the Core ML format.
###
Core ML Tools
https://apple.github.io/coremltools/docs-guides/
###
Once a model is on a person’s device, you can use Core ML to retrain or fine-tune it on-device, with that person’s data.

Core ML optimizes on-device performance by leveraging the CPU, GPU, and Neural Engine while minimizing its memory footprint and power consumption. 
Running a model strictly on a person’s device removes any need for a network connection, which helps keep a person’s data private and your app responsive.

The framework is the foundation for domain-specific frameworks and functionality. 
It supports Vision for analyzing images, Natural Language for processing text, Speech for converting audio to text, and Sound Analysis for identifying sounds in audio. 
Core ML itself builds on top of low-level primitives like Accelerate and BNNS, as well as Metal Performance Shaders.


iOS 11.0
Framework
Vision
Apply computer vision algorithms to perform a variety of tasks on input images and video.
https://developer.apple.com/documentation/vision


iOS 12.0
Framework
Natural Language
Analyze natural language text and deduce its language-specific metadata.
https://developer.apple.com/documentation/naturallanguage


iOS 10.0
Framework
Speech
Perform speech recognition on live or prerecorded audio, and receive transcriptions, alternative interpretations, and confidence levels of the results.
https://developer.apple.com/documentation/speech


iOS 13.0
Framework
Sound Analysis
Classify various sounds by analyzing audio files or streams.
https://developer.apple.com/documentation/soundanalysis


BNNS
Implement and run neural networks for training and inference.
https://developer.apple.com/documentation/accelerate/bnns


iOS 9.0
Framework
Metal Performance Shaders
Optimize graphics and compute performance with kernels that are fine-tuned for the unique characteristics of each Metal GPU family.
https://developer.apple.com/documentation/metalperformanceshaders


iOS 4.0
Technology
Accelerate
Make large-scale mathematical computations and image calculations, optimized for high performance and low energy consumption.
https://developer.apple.com/documentation/accelerate


==================================================
iOS 11.0
class MLModel : NSObject
An encapsulation of all the details of your machine learning model.

MLModel encapsulates a model’s prediction methods, configuration, and model description.

In most cases, you can use Core ML without accessing the MLModel class directly. 
Instead, use the programmer-friendly wrapper class that Xcode automatically generates when you add a model (see Integrating a Core ML Model into Your App). 
If your app needs the MLModel interface, use the wrapper class’s model property.

With the MLModel interface, you can:

Make a prediction with your app’s custom MLFeatureProvider
by calling prediction(from:) or prediction(from:options:).

Make multiple predictions with your app’s custom MLBatchProvider
by calling predictions(fromBatch:) or predictions(from:options:).

Inspect your model’s metadata and MLFeatureDescription instances through modelDescription.
If your app downloads and compiles a model on the user’s device, you must use the MLModel class directly to make predictions. See Downloading and Compiling a Model on the User’s Device.
https://developer.apple.com/documentation/coreml/downloading_and_compiling_a_model_on_the_user_s_device

Important
Use an MLModel instance on one thread or one dispatch queue at a time. 
Do this by either serializing method calls to the model, or by creating a separate model instance for each thread and dispatch queue.