https://developer.apple.com/documentation/realitykit/creating-3d-content-with-reality-composer#Get-the-Reality-Composer-App
https://developer.apple.com/documentation/realitykit/selecting-an-anchor-for-a-reality-composer-scene
https://developer.apple.com/documentation/realitykit/adding-elements-to-a-3d-scene
https://developer.apple.com/documentation/realitykit/arranging-elements-in-a-scene


###
https://developer.apple.com/documentation/realitykit/realitykit-reality-composer
###
RealityKit/Reality Composer
A visual editor for RealityKit AR scenes.
Reality Composer lets you create, compose, and edit RealityKit content visually for iOS, macOS, tvOS, and iPadOS apps.
For developing visionOS apps, use Reality Composer Pro.
For more information, see doc:com.apple.documentation/documentation/visionos/designing-realitykit-content-with-reality-composer-pro.

==================================================Scene Creation
----------Creating 3D Content with Reality Composer
Add more actions by clicking the action sequence Plus button.
New actions occur sequentially by default, but you can make actions occur simultaneously by dragging one onto the other.

See Your Content in the Real World:
If you compose on a Mac, you can synchronize your session between the Mac and an iOS device over a local area network connection.
Make sure Reality Composer is running on the device, and then click the Edit on iOS button in the Toolbar on your Mac.
Select the iOS device from the list of devices that appear.

Add the Composition to Your App:
After you finish your composition, you can add it to your RealityKit-enabled app. Save the composition as an .rcproject file by clicking File > Save in the menu.
Drag this file directly into the navigation pane of your Xcode project.

Xcode automatically generates code that you use to manipulate the stored objects.
For example, Xcode generates a class for the scene based on the scene’s name (MyGreatScene) inside an enumeration based on the name of the project file (MyProject.rcproject).
As a convenience, it also provides a loadMyGreatScene() method:
public enum MyProject {
    public class MyGreatScene: Entity, HasAnchoring { ... }
    public static func loadMyGreatScene() throws -> MyGreatScene { ... }
    ...
}

In your code, you call the load method to get the Reality Composer scene’s anchor. You then add the anchor to your AR view (ARView) scene’s anchor collection.
All of the objects and behaviors that you define in Reality Composer exist as children of the anchor entity, and therefore appear in your app when you display the AR view.

guard let anchor = try? MyProject.loadMyGreatScene() else { return }
arView.scene.anchors.append(anchor)

Export the Composition for Previewing:
You can also save your composition to a .reality or .usdz file for use as a lightweight AR Quick Look experience in your app or on the web.
This allows users to place and preview content in the real world to get a quick sense of what it’s like.

To create a Reality file, choose File > Export > Export Project in the Reality Composer menu, and provide a name for the file.
You use the Reality file that gets stored to disk just like you would use a usdz file, as described in Previewing a Model with AR Quick Look.

To create a USDZ file, see Exporting a Reality Composer Scene to USDZ.

----------Loading Reality Composer Files Using Generated Code
Leverage automatically generated code to load scenes from Xcode.

When you add Reality Composer to your project, Xcode automatically generates code to handle many common tasks, such as loading scenes, retrieving objects, and triggering behaviors.
When you build in Xcode, it converts any Reality Composer file in the current target into a more compact, read-only format called a Reality file, which has the extension .reality.
You can also export Reality files directly from Reality Composer to use, for example, as downloadable content for your app.
Although Xcode accepts both file types, you’ll typically use these editable Reality Composer project files during development.

Xcode generates both asynchronous and synchronous load methods.
Unless your Reality Composer scene is very small, you should use the asynchronous method, which loads scenes in the background. 
Synchronous loading always occurs on the main thread, so your app may become unresponsive if you use that method to load even moderately complex scenes.

Load Reality Files Asyncronously Using Generated Code:
For every scene in a Reality file, Xcode generates an asynchronous loading method.
This method helps keep your app responsive when loading larger scenes.
Xcode bases the name of each generated asynchronous loading method on the name of the scene it loads and the Reality file that contains it.
Instead of returning the loaded object directly, these asynchronous loading methods return the loaded scene through a completion block.
The following example code loads a scene using the generated asynchronous load function:

Experience.loadBoxAsync(completion: { (result) in
    do {
        let boxScene = try result.get()
        // ...
    } catch {
        // handle error
    }
})

Load Reality Files Synchronously Using Generated Code:
Xcode also creates a synchronous loading method for each scene in your Reality Composer projects.
As with generated asynchronous methods, it bases the name of each generated method on the name of the scene it loads and the Reality Composer file that contains that scene.

The following example code synchronously loads a scene called Box from a Reality Composer project called Experience:
// Load the Box scene synchronously from the Experience Reality file
do {
    let boxAnchor = try Experience.loadBox()
    // ...
} catch {
    // handle error
}

Important

Use RealityKit’s synchronous loading methods from the main thread only.
RealityKit throws a runtime exception if you attempt to use it from any other thread.
If you need to load a scene on a background thread, use asynchronous loading instead.

Show the Scene:
After you load the Reality Composer scene, you display it to the user by adding it to the scene anchors of an ARView.
Reality Composer scenes contain anchoring information, which allows RealityKit to automatically find an appropriate surface, object, or image, and anchor your scene to it.
For example, RealityKit automatically places a horizontally anchored scene as soon as ARKit detects a flat horizontal surface.
Similarly, it displays a scene with an image anchor when ARKit detects the specified image at the correct size.

To use automatic anchoring, add the loaded scene to the view’s anchors:

arView.scene.anchors.append(anchor)
For more control, you can also anchor your scene manually. See Taking Control of Scene Anchoring.

Remove the Scene:
To remove the Reality Composer scene so the user can no longer see or interact with it, remove the object from the view’s anchors.
arView.scene.anchors.remove(anchor)
To remove all content from the view, use the removeAll() method instead.
arView.scene.anchors.removeAll()

----------Loading Reality Composer Files Manually Without Generated Code
Load Reality Composer files that aren’t part of your Xcode project.

Although Xcode generates loading methods for all Reality Composer files in your Xcode project, there are times when you need to load them without the benefit of those generated loader methods.
This scenario can happen, for example, when loading content downloaded as part of an in-app purchase bundle from the App Store.

You can manually load scenes synchronously or asynchronously.
Unless your Reality Composer scene is very small, use the asynchronous method, which loads scenes in the background.
Synchronous loading occurs on the main thread, so your app may become unresponsive if you use that method to load even moderately complex scenes.

Create a Scene URL:
Regardless of whether you plan to load the scene synchronously or asynchronously, the first step is to create a URL that points to the Reality file that contains the scene you want to load. Then append the name of the scene to the URL.
Here’s an example of building a URL to load a scene from a Reality file inside your app’s bundle:
func createRealityURL(filename: String, 
                      fileExtension: String, 
                      sceneName:String) -> URL? {
    // Create a URL that points to the specified Reality file. 
    guard let realityFileURL = Bundle.main.url(forResource: filename, 
                                               withExtension: fileExtension) else {
        print("Error finding Reality file \(filename).\(fileExtension)")
        return nil
    }


    // Append the scene name to the URL to point to 
    // a single scene within the file.
    let realityFileSceneURL = realityFileURL.appendingPathComponent(sceneName, 
                                                                    isDirectory: false)
    return realityFileSceneURL
} 

Load the Scene Aynchronously from the URL:
Entity provides the loadAnchorAsync(contentsOf:withName:) method for loading scenes asynchronously from a URL. Loading in this manner uses the Combine framework and returns an AnyCancellable object that you use to cancel scene loading.

You must maintain a strong reference to the returned AnyCancellable object until loading has completed. Otherwise, the load process cancels as soon as the returned AnyCancellable object is deallocated.
To keep a strong reference to the returned AnyCancellable object, store it in an array.

This example shows how to declare an array variable in your view controller or coordinator class to hold instances of AnyCancellable:
var streams = [Combine.AnyCancellable]()
Load your scene asynchronously, storing the AnyCancellable object in that array using the store(in:) method instead of the append(_:) method.
In this way, you ensure that the system won’t cancel your load operation.
This example shows how to asynchronously load a scene from a URL:

func loadRealityComposerSceneAsync (filename: String,
                                    fileExtension: String,
                                    sceneName: String,
                                    completion: @escaping (Swift.Result<(Entity & HasAnchoring)?, Swift.Error>) -> Void) {


    guard let realityFileSceneURL = createRealityURL(filename: filename, fileExtension: fileExtension, sceneName: sceneName) else {
        print("Error: Unable to find specified file in application bundle")
        return
    }


    let loadRequest = Entity.loadAnchorAsync(contentsOf: realityFileSceneURL)
    let cancellable = loadRequest.sink(receiveCompletion: { (loadCompletion) in
        if case let .failure(error) = loadCompletion {
            completion(.failure(error))
        }
    }, receiveValue: { (entity) in
        completion(.success(entity))
    })
    cancellable.store(in: &streams)
}
Note
If you use the store(in:) method instead of append(_:), you don’t have to remove the AnyCancellable object from the array after loading has finished.
When using that method, the AnyCancellable object removes itself automatically from the array once it has finished loading.

Load the Scene Synchronously from the URL:
Call the loadAnchor(contentsOf:withName:) method on Entity and pass in a scene URL.

func loadRealityComposerScene (filename: String,
                                fileExtension: String,
                                sceneName: String) -> (Entity & HasAnchoring)? {
    guard let realitySceneURL = createRealityURL(filename: filename,
                                                 fileExtension: fileExtension,
                                                 sceneName: sceneName) else {
        return nil
    }
    let loadedAnchor = try? Entity.loadAnchor(contentsOf: realitySceneURL)


    return loadedAnchor
}

Load a USDZ File Asynchronously:
You can load USDZ files asynchronously in the background using the loadModelAsync(contentsOf:withName:) method on Entity.
Asynchronous loading of USDZ files also requires the Combine framework. To receive the scene once it’s loaded, the receiving class must conform to the Subscriber protocol.

This example shows you how to conform a view controller to the Subscriber protocol so it can receive the asynchronously loaded scene:
extension ViewController : Subscriber {


    // loadModelAsync loads a ModelEntity, so declare 
    // that as the Input type.
    typealias Input = ModelEntity


    // If loadModelAsync fails, it returns an Error instance, 
    // so declare Error as the Failure associated-object type.
    typealias Failure = Error


    // The publisher has finished, so ask it for the result 
    // as a single item.
    func receive(subscription: Subscription) {
        subscription.request(.max(1))
    }


    // You receive the model here and assign it to a property
    // so you can use it outside of this method. 
    func receive(_ input: ModelEntity) -> Subscribers.Demand {
        // Assign to robot, a property declared in your 
        // main view-controller class
        robot = input
        return .none
    }


    // When the publisher is done - either it has sent the 
    // model or it has errored - this method is called.
    func receive(completion: Subscribers.Completion<Error>) {
        switch (completion) {
            case .failure(let error):
                print("Error loading model: \(error)")
            case .finished:
                print("Model loaded successfully")
        }
    }
}
Once you set up your view controller as a Subscriber, you can start loading a USDZ file as in this example:
let request = Entity.loadModelAsync(contentsOf: robotURL)
request.receive(subscriber: self)
For more information on using the Combine Framework, see Receiving and Handling Events with Combine.
https://developer.apple.com/documentation/Combine
https://developer.apple.com/documentation/Combine/receiving-and-handling-events-with-combine

Load a USDZ File Synchronously:
You can also load individual 3D models from USDZ files for use in your Reality Composer scenes.
This is especially useful, for example, for loading objects that you might spawn multiple times in response to user input.
In that case, you don’t want the asset loaded as part of your Reality Composer scene.
Instead, you want to load it separately and add it to your scene when needed.

You can load a model synchronously using the loadModel(contentsOf:withName:) function on Entity, as in this example:
var loadedModel: Entity?
if let theURL = Bundle.main.url(forResource: "myModel", withExtension: "usdz") {
    let loadedModel = try? Entity.loadModel(contentsOf: theURL)
}

----------Adding Elements to a 3D Scene

----------Configuring Elements in a Scene
Define the appearance and behavior of objects in a scene.

You can configure Reality Composer assets in a number of different ways.
You can assign them unique names, change their appearance, or configure them to simulate real-world physics.

Name the Assets and Scenes:
Reality Composer allows you to assign a name to your scenes, as well as to objects within a scene.
You can use the names in your code to retrieve specific scenes or assets, and Xcode uses the assigned names during code generation.

Reality Composer creates a unique, default name for any newly created scene.
Change the default value to something descriptive to make your code easier to understand.
Tap the scene background to deselect any selected objects, and change the scene’s name using the top-most text field in the inspector.

You can also assign names to a scene’s objects to help differentiate them.
To assign or change an object’s name, tap the object to select it, then type the name into the name field at the top of the Properties inspector.
Xcode automatically generates accessor properties for all named assets.
Unique object names aren’t required, but objects with the same name in the same scene are automatically grouped into a single array property during code generation.

Important
Code generation fails if you use a scene name more than once in the same target.
If you’re using multiple reality files in the same Xcode target, make sure your scene names are unique across all reality files.

Style an Asset to Match Your Scene’s Look:
When you select a built-in Reality Composer asset, the Properties inspector displays a section called Look, where you change the asset’s overall appearance.
In this section, you’ll find either a parameter called Material, for simple shape primitives, or a parameter called Style, for modeled assets.

The Material parameter defines the overall surface appearance of the selected object.
You choose from many predefined physically based rendering (PBR) materials.
Some of these materials, like Glossy Paint, Matte Paint, and Car Paint, allow you to select a color using the Material Color parameter, while others, like Gold and Rubber, use a predefined color.

The Style parameter lets you choose from many predefined appearances.
Many assets, for example, let you choose between Realistic, Stylized, or Iconic.
The realistic style is designed to be a very natural-looking object, often even including scratches or smudges to make the item fit in with its real-world surroundings.
The stylized option creates an object that has fewer real-world details, and the iconic style creates an even less realistic object that makes no attempt to look like it belongs in the real world.

Modify Procedural Asset Parameters in Scene View:
When you use Reality Composer’s built-in assets, you can visually modify some of the configurable parameters in the scene using Reality Composer’s Modify mode.
When in this mode, the manipulator changes to display blue disc-shaped handles that you can use to alter the appearance of the selected asset. The number and effect of these handles varies with the type of asset.
The handles on a cube, for example, allow you to drag different handles to change the cube’s proportions, by separately modifying its height, width, and depth.
To switch to Modify mode on the Mac, select the object you want to edit, choose the Modify button in the toolbar, and select Modify from the Arrange menu or the contextual menu.
On iOS devices, select the object you want to edit, then choose the Modify button in the toolbar.

Use Physics and Gravity in a Scene:
Reality Composer scenes can simulate real-world physics behavior, such as gravity and object collisions.
Objects in a scene that participate in the physics simulation can interact with each other or with detected real-world surfaces, such as tables, walls, and floors.
By default, Reality Composer scenes have physics and gravity enabled, but objects you add to a scene do not participate in the physics simulation unless you specifically configure them to do so.

To enable physics for an object, select it to bring up its Properties inspector.
The bottom-most section in the inspector is labeled “Physics.” For a newly added object, this section contains a single checkbox labeled “Participates.”
Select this checkbox to add this object to the physics simulation.

When you enable the Participates checkbox, three new options appear in the Physics section of the Properties inspector:
Motion Type. Defines how physics work for the selected object.
Two options are available: Fixed and Dynamic.
When an object’s motion type is set to Fixed, other objects in the scene can collide with it, but it stays in the same place, unaffected by those collisions.
When an object’s Motion Type is set to Dynamic, it participates fully in the physics simulation.
If, for example, a dynamic object is not resting on a surface, the scene’s gravity makes it fall until it lands on a surface or another object.
When you apply forces to a dynamic object, or a dynamic object collides with another object, it moves.

Material. Defines how the object behaves when colliding with other objects, basing its behavior on real-world substances like concrete and rubber.
The physics material determines how heavy an object is based on its size.
It also determines how the object reacts when it collides with another object or a detected surface.
A rubber object, for example, bounces higher than a lead object of the same size.

Note
An object’s Material parameter in the Physics section of the inspector is completely distinct from the Material parameter in the Look section.
The physics material defines the properties of the object that are important to the physics simulation, such as the mass and elasticity (or “bounciness”) of the object, while the look material defines how the object is rendered. It is entirely possible to create an asset that looks like one material and behaves like a completely different material.

Collision Shape. Used when RealityKit performs physics simulations on the object.
Because physics calculations for complex shapes are computationally expensive, RealityKit uses a simplified shape like a box or sphere instead of the object’s actual shape when doing physics calculations.
You can select Automatic to let RealityKit choose a collision shape, or you can manually select the shape that will give the behavior you want.

----------Arranging Elements in a Scene
Manipulate objects to complete your Reality Composer scene.

After you add assets to your Reality Composer scene, you can manipulate them in a number of ways to create your final scene.
You assign names to objects and group them together to keep your project and code well organized.
You can reposition, rotate, and scale assets relative to the scene’s anchor guides using the manipulator tool that appears whenever you select one or more objects.
You can also use the Transform section of the Properties inspector to set precise transform values.

Move an Asset:
To reposition an asset using the manipulator, tap or click the object to select it, then drag any of the three cone-shaped handles to move the object along the X (red), Y (green), or Z (blue) axis. You can also drag the object itself to move it up, down, left, or right relative to the current view.

To change the position of an asset using the Properties inspector, type the new X, Y, and Z value corresponding to the desired location using the Position text fields.
You can specify a unit such as cm or m, or you can type just a number, which is interpreted in the currently displayed unit.
For example, if the X position is currently displayed as 10 cm, and you type 100, Reality Composer interprets your input as 100 cm.

Note
Reality Composer uses metric measurements by default.
If you prefer to use imperial measurements such as feet and inches, you can make this change in Reality Composer’s preferences.

Rotate an Asset:
To rotate the selected asset, drag the red, green, or blue manipulator ring corresponding to the axis you want to rotate in the direction you wish to rotate it.
Reality Composer doesn’t display all three rotation rings at the same time.
Instead, it displays one ring based on your current viewing angle.
If the manipulator doesn’t show the ring for the axis you want to rotate, you can either rotate the scene view until the desired ring shows up, or you can tap or click the movement arrow of the same color as the axis you wish to rotate on, which causes the manipulator to show the desired rotation ring. You can also use the inspector to enter precise rotation values for each axis.

Scale an Asset:
To scale the selected asset, drag the rotation ring away from the center of the object to scale up, or toward the center of the object to scale down.
You can also change the scale using the Scale slider in the Properties inspector.

Toggle Snapping to Constrain Transforms:
Toggle snapping by pressing the Snap button in the toolbar.
The Snap option affects how the manipulator works, and it affects each of the transformation types differently.
With position changes, Snap helps you align objects with the ground plane, or the center or edges of other objects based on their bounding boxes.
When rotating an asset, Snap constrains rotation to 15-degree increments. When scaling an asset, the object will snap to 100 percent.

Change the Coordinate Space:
By default, when you reposition an object using the manipulator, you move the object along the axes of the scene’s coordinate system (known as world space).
The X position manipulator, for example, always moves the object along the scene’s X-axis regardless of whether the object has been rotated.

Sometimes, you’ll want to move an object along its own rotated axes instead.
Reality Composer allows you to toggle between the selected object’s coordinate system (known as local space) and the scene’s coordinate system.
Once you’ve switched to local space, the manipulator handles change position as you rotate the object, so that movement happens relative to the object’s current orientation.

To toggle between scene space and object space on a Mac, use the Space button on the toolbar or select the Space item from the Arrange menu.

To toggle between scene space and object space on an iOS device, tap the Settings button in the toolbar, then select either Transform World or Transform Local.

Duplicate an Existing Asset:
You can duplicate any asset in your scene.
Duplicating an existing asset copies all of the parameter values that you’ve set in the Properties inspector except the asset’s position, which is offset from the original asset’s position to make the copy easier to find.

To duplicate an asset on the Mac, click an object in the scene view to select it, then click Duplicate in the Edit menu.

To duplicate an asset on an iOS device, select an object by tapping it in the scene view, then tap the selected object again to bring up a popover menu.
From that menu, choose Duplicate.

Collect Assets into Groups:
You can combine multiple assets in a scene into a group. Grouped objects behave as a single combined object in Reality Composer’s scene view.
Behaviors can target groups as well as individual objects inside a group.
In code, a group is represented by an invisible Entity with each of the grouped objects as its children.

Note
If you’ve enabled physics in your scene, grouped objects still behave as separate, individual objects.

To group items on the Mac, click the first object you want to include in the group to select it, then shift-click additional objects to add them to the selection
Once you select all the objects, choose Group from the Arrange menu.
To ungroup items, select the group by clicking it in the scene view and choosing Ungroup from the Arrange menu.

To group items on an iOS device, touch and hold the first object you want to include in the group.
While continuing to hold with the first finger, use another finger to tap additional objects to add them to the selection.
You can deselect objects by tapping them a second time while continuing to hold your first finger down.
When you’ve selected all the objects to groupe, tap any of the selected objects to bring up an edit menu.
From that menu, choose Group. To ungroup items, select the group by tapping it in the scene view, then tap again to bring up the edit menu, and choose Ungroup.

----------Manipulating Reality Composer Scenes from Code
Make programmatic changes to your scenes at runtime.

Although Reality Composer behaviors provide a way to add interactivity to your scene without writing code, there are times when the only way to implement functionality is to make changes from code.
You can manipulate the objects in your scene in many different ways, including hiding or duplicating them, or changing their location, orientation, and scale.

Access Scene Objects:
To retrieve specific objects from a scene, first assign each object a name in Reality Composer, using the Properties inspector.
If you give an object a unique name, Xcode automatically creates an accessor property during code generation.
For example, if you have an object called Steel Box in a scene named Box that’s contained in a file called Experience.reality, you can get a reference to that object by using code like this:

if let boxScene = try? Experience.loadBox() {
    let box = boxScene.steelBox
    // Do something with box.
}

If you give multipe objects in a scene the same name, the generated property returns an array containing all of the objects that share that name.

If you need to load a named object without using generated code, such as when working with downloaded reality files or loading objects created at runtime, you can use the findEntity(named:) function on the scene entity.

if let boxScene = try? Experience.loadBox() {
    if let box = boxScene.findEntity(named: "Steel Box") {}
        // Do something with box
    }
}

Hide and Show Entities:
In addition to hiding and showing objects in your scene using behaviors, as described in Bringing a Reality Composer Scene to Life, you can also use code to hide and show them, by using the isEnabled property. If you set an object’s isEnabled property to false, ARKit doesn’t render it and it doesn’t participate in the scene’s physics simulation.
If you set it back to true, ARKit starts rendering the object again and it resumes participating in the physics simulation.
Note
Child objects of the isEnabled property inherit its settings, so disabling isEnabled also disables all of its children.

Transform a Scene Object:
You can transform (move, rotate, or scale) objects in a scene by using the transform property of the Entity.

To move an object, change the translation value of the transform. For example, to move a scene object 10 cm along the x-axis, use this:
myEntity.transform.translation += SIMD3<Float>(10, 0, 0)
Similarly, to scale an object, modify the scale of the entity’s transform property. For example, you can double the size of an object in your scene like this:
myEntity.transform.scale *= 2
To rotate an object, make changes to the rotation property of its transform. This code shows you how to rotate an object in your scene by 90° on the z-axis:
// Calculate 90° as radians.
let radians = 90.0 * Float.pi / 180.0

// Create a quaternion that represents a 90° rotation on the z-axis
// and add it to the existing rotation value.
anchorEntity.transform.rotation += simd_quatf(angle: radians, 
                                              axis: SIMD3<Float>(0,0,1))

Clone an Entity:
If you need to create multiple copies of the same object at runtime, avoid loading the entity multiple times, which can cause performance issues.
Instead, load a single copy of the entity before you need it, but don’t add it to your scene. When you need a new instance, call the clone(recursive:) function on the loaded entity, and then add the clone as a child of a scene anchor.

Here’s an example of cloning an anchored entity and adding it to your ARView:
let copy = myEntity.clone(recursive: true)

Add Force to an Object:
If an object in a scene participates in the physics simulation, you can add force to that object from code by using the addForce(_:relativeTo:) method. Provide a vector representing the amount and direction of force to add to the object as the first parameter. Here’s an example of applying force to an object to make it fly away from the camera:

// Adjusting the forceMultiplier value changes the amount of force applied.
let forceMultiplier = simd_float3(repeating: 10)
ball.addForce(simd_float3(x: cameraForwardVector.x,
                          y: cameraForwardVector.y,
                          z: cameraForwardVector.z) * forceMultiplier,
              relativeTo: nil)

----------Adding Procedural Assets to a Scene
Create procedurally generated shape primitives to your Reality Composer scene.

RealityKit can generate several types of shape primitives, such as cubes, spheres, and planes.
Shape primitives are useful for a variety of debugging and development purposes; for example, you might use them as proxies for unfinished assets.
You can also use them instead of artist-created assets for some production purposes.
You might, for example, use a generated sphere instead of an artist-modeled ball when creating a sports game.

Generate a Mesh Resource:
MeshResource: A high-level representation of a collection of vertices and edges that define a shape.
在OpenGL中，"mesh"（网格）是一种表示三维物体表面的数据结构。它由一组顶点、法线、纹理坐标和连接这些顶点的三角形（或其他形状）组成。网格是图形渲染中的关键概念，用于描述物体的外观和形状。
一个典型的三维物体可以被分解成许多小的三角形（或其他多边形），每个三角形都由一组顶点表示。这些顶点在三维空间中定义了物体的形状。为了渲染这些三角形，需要将它们的顶点坐标、法线、纹理坐标等信息传递给图形硬件进行处理。
网格的设计和处理对于渲染出逼真的三维场景和物体至关重要。

Several MeshResource class methods generate instances for different shape primitives. You can use MeshResource instances to create entities for your scene.
This code shows you how to generate a procedural, 5-centimeter sphere MeshResource using a generator method:
let sphereResource = MeshResource.generateSphere(radius: 0.05)
This code shows an example of creating a box MeshResource:
let boxResource = MeshResource.generateBox(size: 0.08)
Note
All MeshResource generator methods require measurement parameters to be expressed in meters.

Define the Appearance of the Procedural Asset:
Create a Material instance that describes the desired color and other aspects of the procedural asset’s appearance.
You typically do so by creating an instance of SimpleMaterial.

This example shows you how to display a generated shape with a shiny, metallic-blue surface:
let myMaterial = SimpleMaterial(color: .blue, roughness: 0, isMetallic: true)

SimpleMaterial: A basic material that you can apply to meshes.
Material: A type that describes the material aspects of a mesh, like color and texture.

Create an Entity from a Mesh Resource:
You can’t add a generated MeshResource asset directly to a RealityKit scene.
Instead, generate a ModelEntity object based on the asset and pass it as a single-element array to specify the material you created in the previous step.
let myEntity = ModelEntity(mesh: myMeshResource, materials: [itemMaterial])

Once you’ve created a ModelEntity that represents your procedural object, place it in your scene by adding it as a descendant of any of your scene’s anchors:
if let anchor = myScene.findEntity(named: "My Anchor Entity") {
    anchor.addChild(myEntity)
}

You can also create a new AnchorEntity to hold your procedural object:
// Create a new Anchor Entity using Identity Transform.
let anchorEntity = AnchorEntity(world: Transform())
// Add the entity as a child of the new anchor.
anchorEntity.addChild(myEntity)
// Add the anchor to the scene.
arView.scene.addAnchor(anchorEntity)

----------Improving the Accessibility of RealityKit Apps
Incorporate assistive technologies in your augmented reality app.

To support assistive technologies such as VoiceOver in your RealityKit apps, set the properties on each visible Entity in your scene.
You can configure accessibility in Reality Composer, and in code.

Note
To see an example of accessibility support in a RealityKit app, check out the doc:controlling-entity-collisions-in-realitykit sample code project.

Configure Accessibility in Reality Composer:
When you create a scene in Reality Composer, you can configure the accessibility properties of your entities right in the properties inspector.
Select one or more entities in your scene and click the Accessibility Enabled checkbox in the properties inspector.
In the Label field, give the entity a name to be used by assistive technologies.
In the Detailed Description field, you can optionally add a more in-depth description.

Configure Accessibility in Code:
You can also configure entities to work with assistive technologies in code.
To enable accessibility support for an Entity, set its isAccessibilityElement property to True and provide a short descriptive name using accessibilityLabel.
If you want to provide a more detaied description, set accessibilityDescription.

Because these properties were introduced in iOS 14, any code that sets or reads their values should be wrapped in an availability macro if your project’s deployment target is iOS 13.
Setting these values on an older version of iOS results in a runtime exception.

if #available(iOS 14.0, *) {
    ball.isAccessibilityElement = true
    ball.accessibilityLabel = "a bowling ball"
    ball.accessibilityDescription = "Tap and drag to roll the ball towards the pins."
}

----------Taking Control of Scene Anchoring
Create a more interactive user experience by choosing exactly where to anchor Reality Composer scenes.
You might sometimes want to take direct control over anchoring in a Reality Composer scene.
If, for example, multiple surfaces or objects in your scene are appropriate as anchors, you could let the user choose where to anchor the scene.
Alternatively, you could create an algorithm that determines which of the available anchors to use, such as selecting the anchor closest to the center of the screen or the anchor that’s closest to the user.
To allow for direct control over anchoring in a scene, you load it without anchoring information and then anchor it manually.

Load the Scene Without Anchoring Information:
Use one of these options:
For synchronous loading, use load(contentsOf:withName:) instead of loadAnchor(contentsOf:withName:).
For asynchronous loading, use loadAsync(contentsOf:withName:) instead of loadAnchorAsync(contentsOf:withName:).
func loadUnanchoredScene (filename: String,
                          fileExtension: String,
                          sceneName: String) -> (Entity & HasAnchoring)? {
    guard let realitySceneURL = createRealityURL(filename: filename,
                                                 fileExtension: fileExtension,
                                                 sceneName: sceneName) else {
        return nil
    }
    let loadedScene = try? Entity.load(contentsOf: realitySceneURL)


    return loadedScene
}

Anchor the Scene Manually:
When you load scenes without anchoring information, you can’t add them directly to a scene’s anchors.
Instead, manually create a new AnchorEntity at the desired location, add your loaded scene as a child of the AnchorEntity, and then add the scene to the ARView scene anchors.
For example, if you want to place your scene where the user taps, you can use raycast(from:allowing:alignment:) to look for a suitable surface, and then place it on the tapped surface if one exists.
@discardableResult
func addUnanchoredEntityWhereTapped(_ entity: Entity, 
                                    _ touchPoint:CGPoint)  -> Bool {
    let results = arView.raycast(from: touchPoint, 
                                 allowing: .estimatedPlane, 
                                 alignment: .horizontal)
        if let result = results.first {
            let anchorEntity = AnchorEntity(world: result.worldTransform)
            anchorEntity.addChild(entity)
            arView.scene.addAnchor(anchorEntity)      
            return true
        }
    return false
}

==================================================Anchors
----------Selecting an Anchor for a Reality Composer Scene:
Note
You can manually load and anchor Reality Composer scenes using code, like you do with other ARKit content.
When you anchor a scene in code, RealityKit ignores the scene’s anchoring information.

Select Horizontal Anchoring for Tables and Floors
Horizontal anchoring is the default option when creating a new document or scene.
Use this anchor type for scenes containing 3D objects to be placed on a table, floor, or other flat surface. For more information, see
When you select a horizontal anchor for your scene, Reality Composer shows a guide grid to represent the real-world surface onto which it will place your scene.

Select Vertical Anchoring for Walls and Free-Floating Objects
Use vertical anchoring for scenes that contain objects to be placed against a wall, column, or other vertical surface.
A vertical anchor is a good choice for scenes that contain, for example, a shelf, a basketball hoop, or a wall clock.
After you select this anchor type, Reality Composer displays a vertical grid that represents the wall or other detected vertical surface to position the scene against.

Select Image Anchoring to Augment Real-World 2D Images
Select image anchoring to place your scene over or near an existing two-dimensional image in the real world, such as a poster, painting, or photograph.
In the Properties inspector, select a reference image asset that you want ARKit to detect.
The white square then changes to the selected reference image, indicating that ARKit will look for that specific image and automatically anchor your scene to it.
ARKit continues tracking the image’s location so that if the image moves, your scene moves with it.
In the scene, you should also select the real-world size of the image you want ARKit to detect, by using the Physical Height and Physical Width controls in the Scene inspector.
Important
RealityKit’s automatic anchoring and tracking will not work for your scene if you do not select a reference image.
However, you may wish to leave the reference image empty for some scenes.
If, for example, you wish to attach the same scene to multiple reference images, you will need to configure ARKit’s image detection and then anchor your scene manually.
https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/detecting_images_in_an_ar_experience

Select Face Anchoring to Place a Scene Relative to a Detected Face
Select face anchoring to place content on or near detected human faces in ARKit.
When you select this anchor type, Reality Composer displays a 3D face to stand in for the real-world face that ARKit has detected.
You can’t move or edit the face guide, but the contents of your scene are placed into the real world based on their relative position to the guide face.
If, for example, you put a box in front of the eyes of the guide face, a bar is placed in front of a detected person’s eyes in your app’s ARView.
For more details about ARKit face detection, see Tracking and Visualizing Faces.
https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/tracking_and_visualizing_faces
Note
Face anchoring is only available when using the front-facing camera.

Select Object Anchoring to Place a Scene Near Detected Objects
Select this anchor type to place your scene near a real-world object that ARKit has detected, based on an object that you’ve scanned.
RealityKit automatically detects objects matching your scan, and anchors your scene to it.
Object anchoring is a good choice for adding content to a real-world object like a toy, tool, or other physical object.
After you select object anchoring, Reality Composer displays a yellow cube with a red exclamation point inside.
The cube represents the object your scene is anchored to, and the exclamation point indicates that you need to identify the type of detected object to anchor your scene to.
In the Properties inspector, below the anchor type, select a scanned object asset to use as an anchor.
An image of the scanned object then replaces the exclamation point.
Unlike RealityKit image anchoring, object anchoring does not continuously track detected objects.
If a detected object moves, it may take a few seconds before the scene’s position is updated to reflect its anchor’s new location.

----------Building Custom Behaviors:
Create custom behaviors to control objects in your scene.

Behaviors are composed of two parts: a trigger and an action sequence.
A behavior’s trigger defines how and when it fires, and the action sequence defines what actually happens when it does.
Common triggers include an object being tapped by the user, coming into proximity with another object in the scene, or receiving a notification sent from code.
Common actions include showing, hiding, and moving objects, and playing sounds.

Behaviors give you a versatile way to add interactivity to a Reality Composer scene.
To create your own new behavior, select the Custom option when adding a behavior.
The behavior is added to your scene with no defined trigger or action sequence.

Define a Trigger
Every behavior must contain a trigger, which defines how and when that behavior fires.
https://developer.apple.com/documentation/realitykit/creating-a-trigger

Build an Action Sequence
A behavior’s action sequence defines what actually happens when the behavior fires.
The sequence can consist of one or more actions that you configure to run sequentially or concurrently. 
https://developer.apple.com/documentation/realitykit/adding-interactivity-to-behaviors

Add Actions
To add the first action to a sequence, tap or click the plus button next to the Action Sequence title, or in the dotted box below it, and choose one of the available actions from the popover that appears.
If objects in your scene are selected when you add an action, the new action is automatically configured to affect the selected objects.

If no objects are selected when you create the action, specify the objects that you want it to affect by using the Affected Object section at the top of the action.
Tap or click the Choose button to go into selection mode, then click or tap the object or objects you want the action to affect.
If an object is already selected, clicking or tapping it removes it from being affected by the action. When you’re finished, press the Done button to finalize your selection.

Configure Multiple Actions to Run Concurrently
For many behaviors, a single action is all you’ll need, but you can also create complex action sequences by combining multiple actions in one sequence.
By default, when you add another action to a sequence, it runs sequentially after the existing actions finish.

Sometimes, you want two or more actions to run at the same time instead of running one after the other.
For example, you might want one object to flip at the same time that a sound plays.
To make multiple actions in a sequence run concurrently, drag one of the actions onto the other.
The two actions’ boxes then combine into a single larger box.
To run additional actions simultaneously, drag more actions onto the larger box.
To make concurrent actions sequential again, drag individual actions outside of the joined box.

==================================================Behaviors
----------Bringing a Reality Composer Scene to Life
Add animation and handle user input by using behaviors, triggers, and actions.
Users can interact with your Reality Composer scenes in a number of different ways without you having to explicitly code for them using Reality Composer’s behaviors.
You can, for example, allow a user’s touch to affect objects in your scene, or let objects respond when the user moves close to them.

Reality Composer includes a number of behavior presets that let you easily add common forms of interactivity to your scene.
You can also build your own custom behaviors or use a preset as a starting point for building more complex behaviors.

Add a Behavior Preset
Reality Composer provides six behavior presets that simplify adding common forms of interactivity to your scenes.
If any objects in your scene are selected when you add a behavior from a preset, the new behavior is automatically configured to affect the selected object or objects.
This list describes each of the behavior presets:
Tap & Flip
Causes one or more objects to animate by jumping up and spinning around in response to user taps

Tap & Play Sound
Plays a sound in response to user taps

Tap & Add Force
Applies a force to objects that simulate physics in response to user taps

Start Hidden
Causes one or more objects to be invisible when the scene starts

Wait & Show
Causes one or more objects to start as invisible, then become visible after a specified duration

Proximity & Jiggle
Causes one or more objects to jiggle in response to the viewer getting close to it

To test the behavior, select the Play button in the toolbar, then tap the selected object.

Rename a Behavior
A newly added behavior is named “Behavior.” Immediately rename behaviors to reflect what they actually do.
On an iOS device, press and hold the behavior’s name and select Rename from the menu that appears.
On a Mac, select a behavior, and choose Rename from the Edit menu or press the Return key.

----------Creating a Trigger
Define when and how a behavior fires.
Reality Composer supports five types of triggers: Tap, Scene Start, Proximity To Camera, Collide, and Notification.
In Reality Composer’s Behavior pane, you click or tap inside the dashed box representing a trigger to specify the set of conditions or events that cause that trigger’s behavior to fire.

1.Respond to Taps and Touches
Select the Tap trigger when you want a behavior to fire in response to the user tapping a specific object or objects.
After adding a tap trigger to the behavior, select the object or objects in your scene that you want the user to interact with.
If you accidentally select an object that you don’t want, click or tap it a second time to deselect it. When you’re finished making your selection, press the Done button.
To make further changes to the affected objects, press the Choose button to return to selection mode.

Tip
You can only use tap triggers for detecting single taps with one finger. To detect more complex scenarios, such as double-taps, long presses, or multiple-finger taps, use a notification trigger and then use Gesture Recognizers or SwiftUI gesture detection.
For more information, see Fire a Behavior from Code.

2.Fire a Behavior Immediately
Select the Scene Start trigger if you want a behavior to fire as soon as your scene is loaded and displayed to the user.
You would commonly used this type of trigger to set the starting state; for example, to hide objects you don’t want visible at first, or to initiate animations to run immediately.

3.Fire Behaviors when the User Is Near an Object
Select the Proximity To Camera trigger if you want a behavior to fire whenever the camera—and, therefore, the user—gets close to specific objects in your scene.
Specify how close the user must get by using the Distance slider on the trigger.
If you then select the trigger, a green sphere is displayed around the affected objects to represent the specified distance.
You can also drag the sphere to change the distance visually.

4.Respond to Object Collisions
Select the Collide trigger if you want a behavior to fire whenever specific objects collide with other objects in your scene, or when objects collide with detected real-world surfaces.
Note
The target of a collide trigger must participate in your scene’s physics simulation.
If you select an object that does not participate, Reality Composer prompts you to enable it.

5.Fire a Behavior from Code
Select a Notification trigger to fire a behavior from code you write in Xcode. 
Notification triggers are a good option when none of the other trigger types meet your needs.
When you create a notification trigger, Xcode automatically generates the code needed to activate the behavior.

To trigger a notification, call the post() function on the generated notification.
If, for example, your scene has a Notification trigger called SpinBox, as shown in the figure below, trigger that behavior from your code like this:
myScene.notifications.spinBox.post() 

----------Adding Interactivity to Behaviors
Define what a behavior does by using actions.

Reality Composer provides many actions that you can use as building blocks when constructing a behavior’s action sequence.
Common actions include moving objects in your scene, animating objects in a variety of ways, playing sounds, adding force to objects that participate in the physics simulation, and transitioning to a completely different scene.
You can also call functions written in Xcode to accomplish additional tasks beyond what the built-in actions can do.

Move, Rotate, or Scale an Object from a Behavior:
Behaviors can transform objects to a specific location, rotation, and scale using the Move, Rotate, Scale To action, which animates objects smoothly to a specified world position, orientation, and size.
The end result of using a Move, Rotate, Scale To action is the same regardless of its targets’ starting transform values.
For example, if you create a behavior with a Move, Rotate, Scale To action configured with a scale value of 0.5, all of its targeted objects end up with a scale value of 0.5, regardless of what their starting scale value was.

Behaviors can also transform objects relative to their starting position, orientation, and size using the Move, Rotate, Scale By action.
With this action, the values you enter into the transform fields are applied to the objects’ current transform values instead of being used as absolute values.
As a result, a Move, Rotate, Scale By action has a different effect when run against objects with different starting transform values.
The action’s position and rotation values are added to an object’s starting position and rotation values, while its scale value is multiplied by the object’s starting scale value.
For example, a Move, Rotate, Scale By action with a Location X value of 5 cm results in each of its target objects moving 5 centimeters from its starting position along the scene’s X-axis.
Similarly, configuring a Move, Rotate, Scale By action with a scale value of 0.5, and running it against an object with a starting scale value of 10.0, results in an object with a scale value of 5.0 when the action finishes. That same action run against an object with a starting scale value of 2.0, however, results in an object with a scale value of 1.0 after the behavior runs.

Draw Attention to an Object:
To draw attention to an object, use the Emphasize action, which causes one or more objects in your scene to animate in place in a variety of ways, such as shaking, flipping, jiggling, spinning, or bouncing.

Add Force to an Object:
Behaviors can add forces to any object in your scene that participates in the physics simulation, by using the Add Force action to simulate the effects of being pushed, kicked, tossed, or blown by wind.
Once you’ve selected the object or objects that the force applies to, a manipulator that represents the direction and intensity of the force to be added appears in your scene. To change the direction of the force, move the blue cone or rotate the blue ring.
To change the amount of force to apply, use the Velocity slider on the action.

Play Sounds from a Behavior:
There are three actions that play audio files in Reality Composer scenes: Play Sound, Play Ambience, and Play Music. All three actions function similarly, but are designed for slightly different purposes:
The Play Sound action plays a sound as if it were coming from a specific object in your scene. The volume of the sound changes based on the camera position relative to the object, and whether the camera is pointed toward or away from it. When the user is near the object, the sound plays louder, and when the user is far from the object, the sound is softer.
The Play Ambience action plays sounds anchored to your scene. These sounds track change in orientation but don’t attenuate, so the volume of an ambient sound doesn’t change based on the user’s position relative to any object in the scene. Play Ambience actions are designed for playing background noise that sounds as if it’s coming from far away.
The Play Music action plays sounds at a constant volume, completely independent of the user’s position and orientation, and is designed for playing background music and voice narration.

Transition to Another Scene:
For games and complex AR experiences, you might choose to split up your app’s content into multiple scenes to keep things better organized and to reduce load time and the amount of memory required. You might, for example, choose to have a separate scene for every level in a game.

To switch to a different scene, use a Change Scene action, specifying the new scene you wish to load.
To reset the current scene to its starting state, select the current scene instead of a different scene.

Call Code from a Behavior:
To call code you’ve written in Xcode from a behavior, use the Notify action. Notify actions have a single configurable parameter called Identifier that is used to uniquely identify the notification this action sends. By default, a Notify action’s identifier is the same as the behavior name, but you can change it to any string value that makes sense for your project.
Xcode’s code generation uses the identifier value to automatically create a notification that you use to respond to this action.

To write code that responds to a Notify action, first write a code function that takes an Entity parameter.
This function should contain the code that you want your behavior to call:

func handleTapOnEntity(_ entity: Entity?) {
    guard let entity = entity else { return }        
    // Do something with entity...
}

Once you have your function, you register it as the action’s onAction handler.
Because the Notify action in the preceding figure specifies an identifier value of Tapped, Xcode automatically creates an action called tapped on the class representing its scene.
Assign the new function as the generated action’s onAction handler to call that function whenever the behavior is triggered.
// Register an onAction handler for the notification
// action you created in Reality Composer
boxAnchor.actions.tapped.onAction = handleTapOnEntity(_:)

==================================================Asset creation
----------Object capture
Create 3D objects from a series of photographs using photogrammetry.
In iOS 17 and macOS 12 and later, you can create 3D objects from photographs using a process called photogrammetry.
You provide RealityKit Object Capture with a series of well-lit photographs taken from many different angles.
It analyzes the overlap area between different images to match up landmarks and produces a 3D model of the photographed object.
https://developer.apple.com/documentation/realitykit/realitykit-object-capture

----------USD Assets
Import and use 3D scenes by importing USD files.
RealityKit can import .usd, .usda, .usdc, and .usdz files.
https://developer.apple.com/documentation/realitykit/realitykit-usd-assets

Exporting a Reality Composer Scene to USDZ:
Save a scene or project as USDZ to read, collect, or display in an app or website.

Export a scene or project from Reality Composer as USDZ to display it in an AR or 3D app.
You can also preview a USDZ file in an AR app or website using AR Quick Look.
Because USDZ is a universal file format, you can open a USDZ file in third-party tools that support it and make changes to your scene or project.

Enable the Export Preference
To enable USDZ exports, open Reality Composer’s Preferences menu and check the “Enable USDZ export” option under Content.

Create a USDZ File
From within an open Reality Composer project, choose the File menu > Export option.
In the export sheet, choose whether to export the scene or project in USDZ format and click the Export button.

Note
Reality Composer doesn’t support exporting a scene that contains reimported .reality files.

Reality Composer extends the USD specification to include most of the features that .reality or .rcproject files support, such as anchors, behaviors, and physics.
Because these features aren’t available in the current USD specification, Reality Composer’s USDZ exports provide more functionality on Apple devices than USD files created with third-party tools. 

Note
USDZ exports support horizontal, vertical, image, and face anchors. Apple’s custom USD schema doesn’t support object anchors.

Validating feature support for USD files
https://developer.apple.com/documentation/realitykit/validating-usd-files


==================================================Entity
An element of a RealityKit scene to which you attach components that provide appearance and behavior characteristics for the entity.

You create and configure entities to embody objects that you want to place in the real world in an AR app.
You do this by adding Entity instances to the Scene instance associated with an ARView.

RealityKit defines a few concrete subclasses of Entity that provide commonly used functionality.
For example, you typically start by creating an instance of AnchorEntity to anchor your content, and add the anchor to a scene’s anchors collection.
You might then instantiate a ModelEntity to represent a physical object in the scene, and add that as a child entity to the anchor.
You can also create custom entities, and add them either to an anchor, or as children of another entity.

For example, the Transform component contains the scale, rotation, and translation information needed to position an entity in space.
You store components in the entity’s components collection, which can hold exactly one of any component type.

All entities inherit a few common components from the Entity base class: the Transform component for spatial positioning, and SynchronizationComponent, which enables synchronization of the entity among networked peers.
Specific types of entities define additional behaviors.
For example, the model entity has components for visual appearance (ModelComponent), collision detection (CollisionComponent), and physics simulations (PhysicsBodyComponent and PhysicsMotionComponent).

