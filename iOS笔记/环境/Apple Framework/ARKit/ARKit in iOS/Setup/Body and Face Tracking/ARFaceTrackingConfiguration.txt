ARFaceTrackingConfiguration
A configuration that tracks facial movement and expressions using the front camera.

iOS 11.0
class ARFaceTrackingConfiguration : ARConfiguration

A face-tracking configuration detects faces within 3 meters of the deviceâ€™s front camera. 
When ARKit detects a face, it creates an ARFaceAnchor object that provides information about a person's facial position, orientation, topology, and expressions.

Face tracking supports devices with Apple Neural Engine in iOS 14 and iPadOS 14 and requires a device with a TrueDepth camera on iOS 13 and iPadOS 13 and earlier.
To determine whether the device supports face tracking, call isSupported on ARFaceTrackingConfiguration before attempting to use this configuration.

When you enable the isLightEstimationEnabled setting, a face-tracking configuration estimates directional and environmental lighting (an ARDirectionalLightEstimate object) by referring to the detected face as a light probe.

Note:
Because face tracking provides your app with personal facial information, your app must include a privacy policy describing to users how you intend to use face tracking and face data. 
For details, see the Apple Developer Program License Agreement.
https://developer.apple.com/support/terms/