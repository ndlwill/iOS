https://developer.apple.com/documentation/

https://zhuanlan.zhihu.com/p/27846162
https://zhuanlan.zhihu.com/p/85823265

https://zhuanlan.zhihu.com/p/31631790

iOS11

增强现实
Augmented reality (AR) describes user experiences that add 2D or 3D elements to the live view from a device’s sensors in a way that makes those elements appear to inhabit the real world.
ARKit combines device motion tracking, world tracking, scene understanding, and display conveniences to simplify building an AR experience.

==================================================Verifying Device Support and User Permission
ARKit requires iOS 11.0 or later and an iOS device with an A9 or later processor.
Some ARKit features require later iOS versions or specific devices. 
ARKit also uses a device camera, so you need to configure iOS privacy controls so the user can permit camera access for your app.

If the basic functionality of your app requires AR (using the back camera): Add the arkit key in the UIRequiredDeviceCapabilities section of your app's Info.plist file. Using this key makes your app available only to ARKit-compatible devices.
If augmented reality is a secondary feature of your app: Check for whether the current device supports the AR configuration you want to use by testing the isSupported property of the appropriate ARConfiguration subclass.
If your app uses face-tracking AR: Face tracking requires the front-facing TrueDepth camera on iPhone X. Your app remains available on other devices, so you must test the ARFaceTrackingConfiguration.isSupported property to determine face-tracking support on the current device.

Check the isSupported property before offering AR features in your app's UI, so that users on unsupported devices aren't disappointed by trying to access those features.

ARKit automatically asks the user for permission the first time your app runs an AR session.
Your app's Info.plist file must include the NSCameraUsageDescription key.

==================================================ARSession
The object that manages the major tasks associated with every AR experience, such as motion tracking, camera passthrough, and image analysis.

An ARSession object coordinates the major processes that ARKit performs on your behalf to create an augmented reality experience. 
These processes include reading data from the device's motion sensing hardware, controlling the device's built-in camera, and performing image analysis on captured camera images. 
The session synthesizes all of these results to establish a correspondence between the real-world space the device inhabits and a virtual space where you model AR content.

Every AR experience requires an ARSession. If you implement a custom renderer, you instantiate the session yourself.

If you use one of the standard renderers (like ARView, ARSCNView, or ARSKView), the renderer creates a session object for you. When you want to interact with your app's session, you access it on your app's renderer.

Run a Session
Running a session requires a configuration. 
Subclasses of ARConfiguration determine how ARKit tracks a device's position and motion relative to the real world, and thus it determines the kinds of AR experiences you create. 
For example, ARWorldTrackingConfiguration enables you to augment the user's view of the world around them though the device's back camera.

==================================================ARAnchor
An object that specifies the position and orientation of an item in the physical environment.

To track the static positions and orientations of real or virtual objects relative to the camera, create anchor objects and use the add(anchor:) method to add them to your AR session.

Adding an anchor to the session helps ARKit to optimize world-tracking accuracy in the area around that anchor, so that virtual objects appear to stay in place relative to the real world. 
If a virtual object moves, remove the corresponding anchor from the old position and add one at the new position.

Some ARKit features automatically add special anchors to a session. World-tracking sessions can add ARPlaneAnchor, ARObjectAnchor, and ARImageAnchor objects if you enable the corresponding features; 
face-tracking sessions add ARFaceAnchor objects.

==================================================Recording and Replaying AR Session Data
ARKit apps use video feeds and sensor data from an iOS device to understand the world around the device.
This reliance on real-world input makes the testing of an AR experience challenging because real-world input is never the same across two AR sessions.
Differences in lighting conditions, device motion, and the location of nearby objects all change how RealityKit understands and renders the scene each time.

To provide consistent data to your AR app, you can record a session using Reality Composer, then use the recorded camera and sensor data to drive your app when running from Xcode.

Record an AR Session in Reality Composer:
Tap the Settings toolbar button. It’s the circle containing three dots in the upper-right corner.
In the Settings sidebar that appears, tap Developer.
In the Developer window, tap Record AR Session.
Move your iOS device to the initial location for your recording.
Tap the record button to start.

Move your device around until it anchors the Reality Composer scene, then continue moving it around until you’ve captured the desired input.
When done, tap the stop button to end recording.
Rename the recording by providing a custom name in the Capture Complete window, then tap Done to save the recording to Reality Composer’s library.


Replay an AR Session in Reality Composer:
To replay a session capture right after recording it, tap the Replay button in the Capture Complete window.
To replay a session later, follow these steps:
Tap the Settings toolbar button.
Tap Developer in the Settings inspector.
In the Developer window, tap Replay AR Session.
Select the session to replay.
Press the Play button to begin playback.

If the recorded session meets your needs, you can export it to a Quicktime movie file by hitting the Share button from either the Recording Complete window or the Playback Complete window. If your Mac is on and unlocked, you can Airdrop the file directly to your computer.


Use a Recorded Session in an App:
In Xcode, you can specify an exported recording to use when launching your app.
To select a recording, edit your project’s scheme and choose the Run phase from the left pane.
Select the Options tab, then look for a row labeled ARKit with a “Replay data” checkbox next to it.
Check that box, then choose Add Replay Data to Project from the popup button next to it to select the recording.

When you run your app with this option selected, it uses the recorded session instead of the device’s camera and sensors.


"Environment map"（环境贴图）是计算机图形学中的一个概念，它用于模拟环境中的光照和反射效果，以增强渲染的真实感和质感。
###
UIApplication.shared.isIdleTimerDisabled = true
###

==================================================USDZ schemas for AR
Leveraging Pixar’s Universal Scene Description standard, USDZ delivers AR and 3D content to Apple devices.
Apple developed a set of new schemas in collaboration with Pixar to further extend the format for AR use cases.
Simply add data to a USDZ file to give your 3D assets AR abilities, such as the ability to:
Anchor 3D content at a specific location in the real world.
React to real-world situations.
Participate in a physics simulation.
Connect audio effects to a location.
Annotate the environment by displaying text.

数字内容创作（Digital Content Creation，DCC）是指使用计算机技术和软件工具来创建各种类型的数字内容，包括图像、音频、视频、动画、虚拟现实（VR）、增强现实（AR）等。

SchemaDefinitions（模式定义）通常是指在计算机科学和软件工程中，用于定义数据结构、对象属性、接口、函数签名等的结构化描述。这些描述可以用于不同编程语言、数据交换格式、数据库、API 文档等中，以便在不同系统之间共享和理解数据的结构和意义。

在计算机图形学中，"prim" 通常指的是基本几何图元或原始图元（primitive），是构成三维场景和模型的基本元素。"Prim" 是 "primitive" 的缩写，表示一些基本的、不可再分的图形元素。
USD refers to individual units of 3D content as 

The prim for the virtual castle (USD refers to individual units of 3D content as prims; see UsdPrim) instructs the runtime to place the castle on a known image in the physical environment, called the image anchor.
When the user comes into proximity with the anchor, the runtime displays the 3D visualization of the castle.
Falling snowflakes represent additional prims that behave as if in accordance with gravity, and disappear as they approach a real-world surface.


You automatically get Reality Composer for macOS when you install Xcode 11 or later.
The app is one of the developer tools bundled with Xcode. From the menu, choose Xcode > Open Developer Tool, and select Reality Composer.


==================================================线性代数与空间变换
https://blog.csdn.net/qq_31622605/category_8801787.html

Matrix4x4
若仅用于空间计算，则只看矩阵的前三行即可，第一列的（1，0，0）代表X轴，第二列的（0，1，0）代表Y轴，第三列的（0，0，1）代表Z轴，第四列的（0，0，0）代表空间位置。此外，第四行永远为（0，0，0，1）。

==================================================ARKit in iOS
1.Setup
----------Managing Session Life Cycle and Tracking Quality
Keep the user informed on the current session state and recover from interruptions.

class ARCamera : NSObject
Information about the camera position and imaging characteristics for a given frame.
You get camera information from the camera property of each ARFrame ARKit delivers.
var transform: simd_float4x4 { get }
The position and orientation of the camera in world coordinate space.

World-tracking AR sessions use a technique called visual-inertial odometry.
This process combines motion sensor data with computer vision analysis of camera imagery to track the device's position and orientation in real-world space, also known as pose, which is expressed in the ARCamera transform property.
For best results, world tracking needs consistent sensor data and camera imagery with visual complexity or recognizable features.

When you start a session, it takes some time for ARKit to gather enough data to precisely model device pose.
During a session, the conditions that affect world-tracking quality can change.
Use ARSessionObserver delegate methods and ARCamera properties to follow these changes.

Basic Lifecycle of an AR Session:


----------Previewing a Model with AR Quick Look
Display a model or scene that the user can move, scale, and share with others.

AR Quick Look enables the user to place virtual content that you provide on any surface that ARKit finds in the real-world environment.

Choose an Input Format
You provide content for your AR experience in .usdz or .reality format:
To browse a library of .usdz files, see the AR Quick Look Gallery.
https://developer.apple.com/augmented-reality/quick-look/
https://developer.apple.com/augmented-reality/resources/
https://developer.apple.com/augmented-reality/tools/
To browse a library of .reality assets, use Reality Composer.

Note
If you include a Reality Composer file (.rcproject) in your app's Copy Files build phase, Xcode automatically outputs a converted .reality file in your app bundle at build time.

Display an AR Experience in Your App:
In your app, you enable AR Quick Look by providing QLPreviewController with a supported input file.
import UIKit
import QuickLook
import ARKit


class ViewController: UIViewController, QLPreviewControllerDataSource {


    override func viewDidAppear(_ animated: Bool) {
        let previewController = QLPreviewController()
        previewController.dataSource = self
        present(previewController, animated: true, completion: nil)
    }


    func numberOfPreviewItems(in controller: QLPreviewController) -> Int { return 1 }


    func previewController(_ controller: QLPreviewController, previewItemAt index: Int) -> QLPreviewItem {
        guard let path = Bundle.main.path(forResource: "myScene", ofType: "reality") else { fatalError("Couldn't find the supported input file.") }
        let url = URL(fileURLWithPath: path)
        return url as QLPreviewItem
    }    
}

To prevent the user from scaling your virtual content or to customize the default share sheet behavior, use ARQuickLookPreviewItem instead of QLPreviewItem.

Display an AR Experience in Your Web Page:
<div>
    <a rel="ar" href="/assets/models/my-model.usdz">
        <img src="/assets/models/my-model-thumbnail.jpg">
    </a>
</div>
When the user clicks the link in Safari or within a web view that's displayed in your app, iOS presents your scene in an AR Quick Look view on your behalf.
For more information, see Viewing Augmented Reality Assets in Safari for iOS.
https://webkit.org/blog/8421/viewing-augmented-reality-assets-in-safari-for-ios/

----------Specifying a lighting environment in AR Quick Look
Add metadata to your USDZ file to specify its lighting characteristics.

AR Quick Look in iOS 16 and later enhances lighting to deliver more brightness, contrast, and visual definition for your scene's virtual content.
You can set an asset's lighting environment, or image-based lighting (IBL), by adding the preferredIblVersion metadata to the file’s .usda textual definition, or by generating the asset with Apple-provided tools.





----------preferredIblVersion
Metadata that determines the lighting environment of virtual content.
A value of 1 indicates the classic lighting environment, and a value of 2 indicates the new lighting environment.

If you omit the preferredIblVersion metadata or give it a value of 0, the system checks the asset’s creation timestamp.
A timestamp of July 1, 2022, or later results in the new lighting environment; otherwise, the scene features classic lighting for backward compatibility.
The system checks the timestamp of the .usd asset within the .usdz archive, not the archive's file creation date.

Select the scene's image-based lighting
The following .usda definition chooses the new lighting environment:

// asset.usda
#usda 1.0
(
    customLayerData = {
        dictionary Apple = {
            int preferredIblVersion = 2
        }
    }
)

Tip
RealityKit doesn’t observe the preferredIblVersion metadata, but you configure the same lighting environment manually.

----------sceneLibrary
Metadata that partitions an asset into scene-based units.

--------------------------------------------------2.Views
iOS13
@MainActor @objc
class ARView
A view that enables you to display an AR experience with RealityKit.
Use an ARView instance to display rendered 3D graphics to the user.
A view has a single Scene instance that you access through the read-only scene property.
To the view’s Scene instance you add one or more AnchorEntity instances that tell the view’s AR session how to tether content to something in the real world. 
To each anchor, you attach a hierarchy of other Entity instances that make up the content of the scene.

iOS11
class ARSCNView : SCNView
A view that blends virtual 3D content from SceneKit into your augmented reality experience.
The ARSCNView class provides the easiest way to create augmented reality experiences that blend virtual 3D content with a device camera view of the real world.
When you run the view's provided ARSession object:
The view automatically renders the live video feed from the device camera as the scene background.
The world coordinate system of the view's SceneKit scene directly responds to the AR world coordinate system established by the session configuration.
The view automatically moves its SceneKit camera to match the real-world movement of the device.
You don't necessarily need to use the ARAnchor class to track positions of objects you add to the scene, but by implementing ARSCNViewDelegate methods, you can add SceneKit content to any anchors that are automatically detected by ARKit.
Because ARKit requires Metal, use only Metal features of SceneKit. For example:
This class supports only SCNProgram instances with Metal Shading Language code.
If you set the preferredRenderingAPI property to SCNRenderingAPI.openGLES2, the framework reverts the value to SCNRenderingAPI.metal.

iOS11
class ARSKView : SKView
A view that blends virtual 2D content from SpriteKit into the 3D space of an augmented reality experience.

iOS13
class ARCoachingOverlayView : UIView
A view that displays standardized onboarding instructions to direct users toward a specific goal.
This view offers your users a standardized onboarding routine.
You can configure this view to automatically display during session initialization and in limited tracking situations, while giving the user specific instructions that best facilitate ARKit's world tracking.
These illustrations show overlay views with horizontal- and vertical-plane goals, indicating that the user should begin moving the device:
These illustrations show overlay views indicating that the user should continue moving the phone or change the speed with which they move it:
When you start your app, the coaching overlay asks the user to move the device in ways that help ARKit establish tracking.
When you choose a specific goal like finding a plane, the view tailors its instructions accordingly.
After the coaching overlay determines the goal has been met and no further coaching is required, it hides from the user's view.
Supporting Automatic Coaching:
By default, activatesAutomatically is enabled and therefore you should override coachingOverlayViewWillActivate(_:) to determine whether coaching is in progress.
Coordinate your actions to help the user focus on these instructions, for example, by hiding any UI that's not necessary while the session reinitializes.
Relocalizing After an Interruption:
If relocalization is enabled (see sessionShouldAttemptRelocalization(_:)), ARKit attempts to restore your session if any interruptions degrade your app's tracking state.
In this event, the coaching overlay presents itself and gives the user instructions to assist ARKit with relocalizing.
During this time, the coaching overlay includes a button that lets the user indicate they'd like to start over rather than restore the session.
ARKit notifies you when the user presses Start Over by calling your delegate's coachingOverlayViewDidRequestSessionReset(_:) function.
Implement this callback if your app requires any custom actions to restart the AR experience.
func coachingOverlayViewDidRequestSessionReset(_ coachingOverlayView: ARCoachingOverlayView) {    


    // Reset the session.
    let configuration = ARWorldTrackingConfiguration()
    configuration.planeDetection = [.horizontal, .vertical]
    session.run(configuration, options: [.resetTracking])


    // Custom actions to restart the AR experience. 
    // ...
}
If you do not implement coachingOverlayViewDidRequestSessionReset(_:), the coaching overlay responds to the Start Over button by resetting tracking, which also removes any existing anchors.
For more information about relocalization, see Managing Session Life Cycle and Tracking Quality.
https://developer.apple.com/documentation/arkit/arkit_in_ios/managing_session_life_cycle_and_tracking_quality


class SCNView : UIView
A view for displaying 3D SceneKit content.

class SKView : UIView
A view subclass that renders a SpriteKit scene.

class ARSession : NSObject
The object that manages the major tasks associated with every AR experience, such as motion tracking, camera passthrough, and image analysis.

--------------------------------------------------3.Virtual Content
----------Content Anchors
Identify items in the physical environment, including planar surfaces, images, physical objects, body positions, and faces.
Anchors identify the position of items in your augmented reality session.
Use anchors to obtain information about the item itself, or about the thing it represents.
For example, use an ARPlaneAnchor to determine the location of a planar surface.

(1)Surface Detection

class ARPlaneAnchor : ARAnchor
An anchor for a 2D planar surface that ARKit detects in the physical environment.

class ARMeshAnchor : ARAnchor
An anchor for a physical object that ARKit detects and recreates virtually using a polygonal mesh.

class ARWorldTrackingConfiguration : ARConfiguration
A configuration that tracks the position of a device in relation to objects in the environment.

class SCNNode : 
A structural element of a scene graph, representing a position and transform in a 3D coordinate space, to which you can attach geometry, lights, cameras, or other displayable content.

ARKit offers two ways to track the area of an estimated plane. A plane anchor’s geometry describes a convex polygon tightly enclosing all points that ARKit currently estimates to be part of the same plane (easily visualized using ARSCNPlaneGeometry).
ARKit also provides a simpler estimate in a plane anchor’s extent and center, which together describe a rectangular boundary (easily visualized using SCNPlane).

class ARAnchor : NSObject
An object that specifies the position and orientation of an item in the physical environment.

class ARMeshAnchor : ARAnchor
An anchor for a physical object that ARKit detects and recreates virtually using a polygonal mesh.

(2)Image Detection



----------Providing 3D Virtual Content with SceneKit:
Use SceneKit to add realistic three-dimensional objects to your AR experience.
Because ARKit automatically matches SceneKit space to the real world, placing a virtual object so that it appears to maintain a real-world position requires that you set the object's SceneKit position appropriately.
For example, in a default configuration, the following code places a 10-centimeter cube 20 centimeters in front of the camera's initial position:

let cubeNode = SCNNode(geometry: SCNBox(width: 0.1, height: 0.1, length: 0.1, chamferRadius: 0))
cubeNode.position = SCNVector3(0, 0, -0.2) // SceneKit/AR coordinates are in meters
sceneView.scene.rootNode.addChildNode(cubeNode)

The code above places an object directly in the view’s SceneKit scene.
The object automatically appears to track a real-world position because ARKit matches SceneKit space to real-world space.

Alternatively, you can use the ARAnchor class to track real-world positions, either by creating anchors yourself and adding them to the session or by observing anchors that ARKit automatically creates. For example, when plane detection is enabled, ARKit adds and updates anchors for each detected plane. To add visual content for these anchors, implement ARSCNViewDelegate methods such as the following:
func renderer(_ renderer: SCNSceneRenderer, didAdd node: SCNNode, for anchor: ARAnchor) {
    // This visualization covers only detected planes.
    guard let planeAnchor = anchor as? ARPlaneAnchor else { return }


    // Create a SceneKit plane to visualize the node using its position and extent.
    let plane = SCNPlane(width: CGFloat(planeAnchor.extent.x), height: CGFloat(planeAnchor.extent.z))
    let planeNode = SCNNode(geometry: plane)
    planeNode.position = SCNVector3Make(planeAnchor.center.x, 0, planeAnchor.center.z)


    // SCNPlanes are vertically oriented in their local coordinate space.
    // Rotate it to match the horizontal orientation of the ARPlaneAnchor.
    planeNode.transform = SCNMatrix4MakeRotation(-Float.pi / 2, 1, 0, 0)


    // ARKit owns the node corresponding to the anchor, so make the plane a child node.
    node.addChildNode(planeNode)
}

Follow Best Practices for Designing 3D Assets:
Use the SceneKit physically based lighting model for materials for a more realistic appearance. (See the SCNMaterial class and the Badger: Advanced Rendering in SceneKit sample code project.)
https://developer.apple.com/library/archive/samplecode/Badger/Introduction/Intro.html#//apple_ref/doc/uid/TP40017309
Bake ambient occlusion shading so that objects appear properly lit in a wide variety of scene lighting conditions.
If you create a virtual object that you intend to place on a real-world flat surface in AR, include a transparent plane with a soft shadow texture below the object in your 3D asset.

class SCNMaterial : NSObject
A set of shading attributes that define the appearance of a geometry's surface when rendered.