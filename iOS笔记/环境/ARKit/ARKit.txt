https://developer.apple.com/documentation/

https://zhuanlan.zhihu.com/p/27846162
https://zhuanlan.zhihu.com/p/85823265

https://zhuanlan.zhihu.com/p/31631790

iOS 11.0

增强现实
#####
Augmented reality (AR) describes user experiences that add 2D or 3D elements to the live view from a device’s sensors in a way that makes those elements appear to inhabit the real world.
ARKit combines device motion tracking, world tracking, scene understanding, and display conveniences to simplify building an AR experience.
#####

==================================================Verifying Device Support and User Permission
ARKit requires iOS 11.0 or later and an iOS device with an A9 or later processor.
Some ARKit features require later iOS versions or specific devices. 
ARKit also uses a device camera, so you need to configure iOS privacy controls so the user can permit camera access for your app.

How to handle device compatibility support depends on how your app uses ARKit:
If the basic functionality of your app requires AR (using the back camera): Add the arkit key in the UIRequiredDeviceCapabilities section of your app's Info.plist file. Using this key makes your app available only to ARKit-compatible devices.
If augmented reality is a secondary feature of your app: Check for whether the current device supports the AR configuration you want to use by testing the isSupported property of the appropriate ARConfiguration subclass.
If your app uses face-tracking AR: Face tracking requires the front-facing TrueDepth camera on iPhone X. Your app remains available on other devices, so you must test the ARFaceTrackingConfiguration.isSupported property to determine face-tracking support on the current device.

Tip:
Check the isSupported property before offering AR features in your app's UI, so that users on unsupported devices aren't disappointed by trying to access those features.

Handle User Consent and Privacy:
For your app to use ARKit, the user must explicitly grant your app permission for camera access. 
ARKit automatically asks the user for permission the first time your app runs an AR session.

iOS requires your app to provide a static message to be displayed when the system asks for camera or microphone permission. 
Your app's Info.plist file must include the NSCameraUsageDescription key. 
For that key, provide text that explains why your app needs camera access so that the user can feel confident granting permission to your app.

Note:
If you create a new ARKit app using the Xcode template, a default camera usage description is provided for you.

If your app uses ARFaceTrackingConfiguration, ARKit provides your app with personal facial information. 
If you use ARKit face tracking features, your app must include a privacy policy describing to users how you intend to use face tracking and face data. 
For details, see the Apple Developer Program License Agreement.


==================================================ARSession
###
The object that manages the major tasks associated with every AR experience, such as motion tracking, camera passthrough, and image analysis.
###

class ARSession : NSObject

An ARSession object coordinates the major processes that ARKit performs on your behalf to create an augmented reality experience. 
These processes include reading data from the device's motion sensing hardware, controlling the device's built-in camera, and performing image analysis on captured camera images. 
The session synthesizes all of these results to establish a correspondence between the real-world space the device inhabits and a virtual space where you model AR content.

Create a Session:
Every AR experience requires an ARSession. If you implement a custom renderer, you instantiate the session yourself.
let session = ARSession()
session.delegate = self

If you use one of the standard renderers (like ARView, ARSCNView, or ARSKView), the renderer creates a session object for you. 
When you want to interact with your app's session, you access it on your app's renderer.
let session = myView.session

Run a Session:
Running a session requires a configuration. 
Subclasses of ARConfiguration determine how ARKit tracks a device's position and motion relative to the real world, and thus it determines the kinds of AR experiences you create. 
For example, ARWorldTrackingConfiguration enables you to augment the user's view of the world around them though the device's back camera.

==================================================ARAnchor
###
An object that specifies the position and orientation of an item in the physical environment.
###
class ARAnchor : NSObject

To track the static positions and orientations of real or virtual objects relative to the camera, create anchor objects and use the add(anchor:) method to add them to your AR session.

Tip:
Adding an anchor to the session helps ARKit to optimize world-tracking accuracy in the area around that anchor, 
so that virtual objects appear to stay in place relative to the real world. 
If a virtual object moves, remove the corresponding anchor from the old position and add one at the new position.

#####
Some ARKit features automatically add special anchors to a session. 
World-tracking sessions can add ARPlaneAnchor, ARObjectAnchor, and ARImageAnchor objects if you enable the corresponding features; 
face-tracking sessions add ARFaceAnchor objects.
#####

Subclassing Notes:
In addition to creating your own ARAnchor instances to track the real-world positions of your virtual content, you can also subclass ARAnchor to associate custom data with anchors you create. 
Ensure that your anchor classes behave correctly when ARKit updates frames or saves and loads anchors in an ARWorldMap:
1. Anchor subclasses must fullfill the requirements of the ARAnchorCopying protocol. ARKit calls init(anchor:) (on a background thread) to copy instances of your anchor class from each ARFrame to the next. Your implementation of this initializer should copy the values of any custom properties your subclass adds.
2. Anchor subclasses must also adopt the NSSecureCoding protocol. Override encode(with:) and init(coder:) to save and restore the values your subclass' custom properties when ARKit saves and loads them in a world map.
3. Anchors are considered equal based on their identifier property.
4. Only anchors that do not adopt ARTrackable are included when you save a world map.


==================================================Recording and Replaying AR Session Data
ARKit apps use video feeds and sensor data from an iOS device to understand the world around the device.
This reliance on real-world input makes the testing of an AR experience challenging because real-world input is never the same across two AR sessions.
Differences in lighting conditions, device motion, and the location of nearby objects all change how RealityKit understands and renders the scene each time.

To provide consistent data to your AR app, you can record a session using Reality Composer, then use the recorded camera and sensor data to drive your app when running from Xcode.

Record an AR Session in Reality Composer:
Tap the Settings toolbar button. It’s the circle containing three dots in the upper-right corner.
In the Settings sidebar that appears, tap Developer.
In the Developer window, tap Record AR Session.
Move your iOS device to the initial location for your recording.
Tap the record button to start.

Move your device around until it anchors the Reality Composer scene, then continue moving it around until you’ve captured the desired input.
When done, tap the stop button to end recording.
Rename the recording by providing a custom name in the Capture Complete window, then tap Done to save the recording to Reality Composer’s library.


Replay an AR Session in Reality Composer:
To replay a session capture right after recording it, tap the Replay button in the Capture Complete window.
To replay a session later, follow these steps:
Tap the Settings toolbar button.
Tap Developer in the Settings inspector.
In the Developer window, tap Replay AR Session.
Select the session to replay.
Press the Play button to begin playback.

If the recorded session meets your needs, you can export it to a Quicktime movie file by hitting the Share button from either the Recording Complete window or the Playback Complete window. If your Mac is on and unlocked, you can Airdrop the file directly to your computer.


Use a Recorded Session in an App:
In Xcode, you can specify an exported recording to use when launching your app.
To select a recording, edit your project’s scheme and choose the Run phase from the left pane.
Select the Options tab, then look for a row labeled ARKit with a “Replay data” checkbox next to it.
Check that box, then choose Add Replay Data to Project from the popup button next to it to select the recording.

When you run your app with this option selected, it uses the recorded session instead of the device’s camera and sensors.


"Environment map"（环境贴图）是计算机图形学中的一个概念，它用于模拟环境中的光照和反射效果，以增强渲染的真实感和质感。
###
UIApplication.shared.isIdleTimerDisabled = true
###

==================================================USDZ schemas for AR
Leveraging Pixar’s Universal Scene Description standard, USDZ delivers AR and 3D content to Apple devices.
Apple developed a set of new schemas in collaboration with Pixar to further extend the format for AR use cases.
Simply add data to a USDZ file to give your 3D assets AR abilities, such as the ability to:
Anchor 3D content at a specific location in the real world.
React to real-world situations.
Participate in a physics simulation.
Connect audio effects to a location.
Annotate the environment by displaying text.

数字内容创作（Digital Content Creation，DCC）是指使用计算机技术和软件工具来创建各种类型的数字内容，包括图像、音频、视频、动画、虚拟现实（VR）、增强现实（AR）等。

SchemaDefinitions（模式定义）通常是指在计算机科学和软件工程中，用于定义数据结构、对象属性、接口、函数签名等的结构化描述。这些描述可以用于不同编程语言、数据交换格式、数据库、API 文档等中，以便在不同系统之间共享和理解数据的结构和意义。

在计算机图形学中，"prim" 通常指的是基本几何图元或原始图元（primitive），是构成三维场景和模型的基本元素。"Prim" 是 "primitive" 的缩写，表示一些基本的、不可再分的图形元素。
USD refers to individual units of 3D content as 

The prim for the virtual castle (USD refers to individual units of 3D content as prims; see UsdPrim) instructs the runtime to place the castle on a known image in the physical environment, called the image anchor.
When the user comes into proximity with the anchor, the runtime displays the 3D visualization of the castle.
Falling snowflakes represent additional prims that behave as if in accordance with gravity, and disappear as they approach a real-world surface.


You automatically get Reality Composer for macOS when you install Xcode 11 or later.
The app is one of the developer tools bundled with Xcode. From the menu, choose Xcode > Open Developer Tool, and select Reality Composer.


==================================================线性代数与空间变换
https://blog.csdn.net/qq_31622605/category_8801787.html

Matrix4x4
若仅用于空间计算，则只看矩阵的前三行即可，第一列的（1，0，0）代表X轴，第二列的（0，1，0）代表Y轴，第三列的（0，0，1）代表Z轴，第四列的（0，0，0）代表空间位置。此外，第四行永远为（0，0，0，1）。

==================================================ARKit in iOS
----------Previewing a Model with AR Quick Look
Display a model or scene that the user can move, scale, and share with others.

AR Quick Look enables the user to place virtual content that you provide on any surface that ARKit finds in the real-world environment.

Choose an Input Format
You provide content for your AR experience in .usdz or .reality format:
To browse a library of .usdz files, see the AR Quick Look Gallery.
https://developer.apple.com/augmented-reality/quick-look/
https://developer.apple.com/augmented-reality/resources/
https://developer.apple.com/augmented-reality/tools/
To browse a library of .reality assets, use Reality Composer.

Note
If you include a Reality Composer file (.rcproject) in your app's Copy Files build phase, Xcode automatically outputs a converted .reality file in your app bundle at build time.

Display an AR Experience in Your App:
In your app, you enable AR Quick Look by providing QLPreviewController with a supported input file.
import UIKit
import QuickLook
import ARKit


class ViewController: UIViewController, QLPreviewControllerDataSource {


    override func viewDidAppear(_ animated: Bool) {
        let previewController = QLPreviewController()
        previewController.dataSource = self
        present(previewController, animated: true, completion: nil)
    }


    func numberOfPreviewItems(in controller: QLPreviewController) -> Int { return 1 }


    func previewController(_ controller: QLPreviewController, previewItemAt index: Int) -> QLPreviewItem {
        guard let path = Bundle.main.path(forResource: "myScene", ofType: "reality") else { fatalError("Couldn't find the supported input file.") }
        let url = URL(fileURLWithPath: path)
        return url as QLPreviewItem
    }    
}

To prevent the user from scaling your virtual content or to customize the default share sheet behavior, use ARQuickLookPreviewItem instead of QLPreviewItem.

Display an AR Experience in Your Web Page:
<div>
    <a rel="ar" href="/assets/models/my-model.usdz">
        <img src="/assets/models/my-model-thumbnail.jpg">
    </a>
</div>
When the user clicks the link in Safari or within a web view that's displayed in your app, iOS presents your scene in an AR Quick Look view on your behalf.
For more information, see Viewing Augmented Reality Assets in Safari for iOS.
https://webkit.org/blog/8421/viewing-augmented-reality-assets-in-safari-for-ios/

----------Specifying a lighting environment in AR Quick Look
Add metadata to your USDZ file to specify its lighting characteristics.

AR Quick Look in iOS 16 and later enhances lighting to deliver more brightness, contrast, and visual definition for your scene's virtual content.
You can set an asset's lighting environment, or image-based lighting (IBL), by adding the preferredIblVersion metadata to the file’s .usda textual definition, or by generating the asset with Apple-provided tools.





----------preferredIblVersion
Metadata that determines the lighting environment of virtual content.
A value of 1 indicates the classic lighting environment, and a value of 2 indicates the new lighting environment.

If you omit the preferredIblVersion metadata or give it a value of 0, the system checks the asset’s creation timestamp.
A timestamp of July 1, 2022, or later results in the new lighting environment; otherwise, the scene features classic lighting for backward compatibility.
The system checks the timestamp of the .usd asset within the .usdz archive, not the archive's file creation date.

Select the scene's image-based lighting
The following .usda definition chooses the new lighting environment:

// asset.usda
#usda 1.0
(
    customLayerData = {
        dictionary Apple = {
            int preferredIblVersion = 2
        }
    }
)

Tip
RealityKit doesn’t observe the preferredIblVersion metadata, but you configure the same lighting environment manually.

----------sceneLibrary
Metadata that partitions an asset into scene-based units.


--------------------------------------------------3.Virtual Content
----------Content Anchors
Identify items in the physical environment, including planar surfaces, images, physical objects, body positions, and faces.
Anchors identify the position of items in your augmented reality session.
Use anchors to obtain information about the item itself, or about the thing it represents.
For example, use an ARPlaneAnchor to determine the location of a planar surface.

(1)Surface Detection

class ARPlaneAnchor : ARAnchor
An anchor for a 2D planar surface that ARKit detects in the physical environment.

class ARMeshAnchor : ARAnchor
An anchor for a physical object that ARKit detects and recreates virtually using a polygonal mesh.

class ARWorldTrackingConfiguration : ARConfiguration
A configuration that tracks the position of a device in relation to objects in the environment.

class SCNNode : 
A structural element of a scene graph, representing a position and transform in a 3D coordinate space, to which you can attach geometry, lights, cameras, or other displayable content.

ARKit offers two ways to track the area of an estimated plane. A plane anchor’s geometry describes a convex polygon tightly enclosing all points that ARKit currently estimates to be part of the same plane (easily visualized using ARSCNPlaneGeometry).
ARKit also provides a simpler estimate in a plane anchor’s extent and center, which together describe a rectangular boundary (easily visualized using SCNPlane).

class ARAnchor : NSObject
An object that specifies the position and orientation of an item in the physical environment.

class ARMeshAnchor : ARAnchor
An anchor for a physical object that ARKit detects and recreates virtually using a polygonal mesh.

(2)Image Detection



----------Providing 3D Virtual Content with SceneKit:
Use SceneKit to add realistic three-dimensional objects to your AR experience.
Because ARKit automatically matches SceneKit space to the real world, placing a virtual object so that it appears to maintain a real-world position requires that you set the object's SceneKit position appropriately.
For example, in a default configuration, the following code places a 10-centimeter cube 20 centimeters in front of the camera's initial position:

let cubeNode = SCNNode(geometry: SCNBox(width: 0.1, height: 0.1, length: 0.1, chamferRadius: 0))
cubeNode.position = SCNVector3(0, 0, -0.2) // SceneKit/AR coordinates are in meters
sceneView.scene.rootNode.addChildNode(cubeNode)

The code above places an object directly in the view’s SceneKit scene.
The object automatically appears to track a real-world position because ARKit matches SceneKit space to real-world space.

Alternatively, you can use the ARAnchor class to track real-world positions, either by creating anchors yourself and adding them to the session or by observing anchors that ARKit automatically creates. For example, when plane detection is enabled, ARKit adds and updates anchors for each detected plane. To add visual content for these anchors, implement ARSCNViewDelegate methods such as the following:
func renderer(_ renderer: SCNSceneRenderer, didAdd node: SCNNode, for anchor: ARAnchor) {
    // This visualization covers only detected planes.
    guard let planeAnchor = anchor as? ARPlaneAnchor else { return }


    // Create a SceneKit plane to visualize the node using its position and extent.
    let plane = SCNPlane(width: CGFloat(planeAnchor.extent.x), height: CGFloat(planeAnchor.extent.z))
    let planeNode = SCNNode(geometry: plane)
    planeNode.position = SCNVector3Make(planeAnchor.center.x, 0, planeAnchor.center.z)


    // SCNPlanes are vertically oriented in their local coordinate space.
    // Rotate it to match the horizontal orientation of the ARPlaneAnchor.
    planeNode.transform = SCNMatrix4MakeRotation(-Float.pi / 2, 1, 0, 0)


    // ARKit owns the node corresponding to the anchor, so make the plane a child node.
    node.addChildNode(planeNode)
}

Follow Best Practices for Designing 3D Assets:
Use the SceneKit physically based lighting model for materials for a more realistic appearance. (See the SCNMaterial class and the Badger: Advanced Rendering in SceneKit sample code project.)
https://developer.apple.com/library/archive/samplecode/Badger/Introduction/Intro.html#//apple_ref/doc/uid/TP40017309
Bake ambient occlusion shading so that objects appear properly lit in a wide variety of scene lighting conditions.
If you create a virtual object that you intend to place on a real-world flat surface in AR, include a transparent plane with a soft shadow texture below the object in your 3D asset.

class SCNMaterial : NSObject
A set of shading attributes that define the appearance of a geometry's surface when rendered.


