Docker 是建立在 Linux 内核机制之上的一个容器管理

Docker 依赖于 Linux 内核特性
Docker 并不是一种虚拟化技术，而是一种操作系统级别的隔离技术，核心依赖 Linux 的以下机制：

| 内核特性                         | Docker 中的作用                      |
| ---------------------------- | -------------------------------- |
| **Namespaces**               | 实现“隔离” → 不同容器看到自己的进程、网络、文件系统等环境  |
| **Cgroups (Control Groups)** | 实现“资源限制” → 限制容器的 CPU、内存、IO 等资源使用 |
| **UnionFS (OverlayFS)**      | 实现“镜像分层” → 通过层叠文件系统实现镜像复用和快速启动   |
| **Capabilities**             | 实现“精细化权限控制”                      |
| **Seccomp/AppArmor/SELinux** | 实现“安全隔离”                         |

没有这些 Linux 特性，Docker 就无法隔离或运行容器。


Docker 运行时有两个主要部分：
dockerd（守护进程）：运行在 Linux 上，负责管理镜像、容器、网络等。
docker CLI（客户端）：命令行工具，向 dockerd 发指令。
+--------------------+
|   Docker CLI       |
+--------------------+
         ↓
+--------------------+
|   Docker Daemon    | ← 直接使用 Linux 内核特性
+--------------------+
         ↓
+--------------------+
|   Linux Kernel     |
+--------------------+


Docker 本质依赖 Linux 内核，那么在 macOS 或 Windows 上怎么运行呢？
其实是通过一个轻量级虚拟机（通常运行一个精简版的 Linux）来实现的
即在非 Linux 系统中，Docker 是“借”一个 Linux 内核来运行的。



#####
容器是一个运行在宿主机（Host）上的、被操作系统内核隔离起来的“独立运行环境”。
#####
你可以把容器想象成：
在同一个 Linux 系统里，被“虚拟出”多个独立的“小操作系统”，每个容器都有自己独立的文件系统、网络、进程空间、用户环境，但共享同一个内核。


虚拟机是“跑多个系统”，
容器是“在一个系统中跑多个独立应用环境”。


容器是从“镜像（Image）”启动的。
镜像相当于一个应用的模板（包含应用 + 依赖 + 配置），而容器是它的运行实例。
就像：
镜像：一个 class 定义；
容器：这个 class 的实例对象。

宿主机（Host） 就是运行容器或虚拟机的那台“真实的计算机”。

| 术语             | 含义                 |
| -------------- | ------------------ |
| **宿主机（Host）**  | 运行容器或虚拟机的“物理或虚拟机器” |
| **客户机（Guest）** | 被宿主机隔离出来运行的容器或虚拟机  |


你自己的 Mac / Windows / Linux 电脑 就可以是宿主机；
它可以运行 Docker 容器（Host → Container）；
或者运行 虚拟机（Host → VM）。


宿主机与容器的关系图
+-----------------------------------------------------------+
|          宿主机 (Host)                                    |
|  +-----------------------------------------------+        |
|  | 操作系统 (Host OS, e.g. Linux)                |        |
|  |  +-------------------------+                  |        |
|  |  | Docker Engine (dockerd) |                  |        |
|  |  +----------+--------------+                  |        |
|  |             |                                 |        |
|  |     +-------v--------+    +-------v--------+  |        |
|  |     |  容器 A        |    |  容器 B        |  |        |
|  |     | 独立文件系统   |    | 独立文件系统   |  |        |
|  |     | 独立网络命名空间 |  | 独立网络命名空间 | |        |
|  |     +----------------+    +----------------+  |        |
|  +-----------------------------------------------+        |
+-----------------------------------------------------------+
这里每个“容器”其实就是宿主机上被隔离出来的一组进程。


#####
容器里只是运行了一组被隔离的进程，
它们共享宿主机的 Linux 内核，
但有自己的文件系统、网络、环境变量等。
#####


容器的网络和宿主机的网络到底是什么关系？
容器的网络其实是由 Linux 内核的 Network Namespace（网络命名空间） 机制隔离出来的，
而 Docker（或其他容器引擎）通过不同的“网络模式”来决定容器与宿主机网络的关系。

核心机制：Network Namespace
在 Linux 里，每个进程可以运行在不同的 Network Namespace 里。
这意味着：
每个容器看到的网卡、IP、路由表都可以是独立的；
但底层还是共用宿主机的网络设备（通过虚拟桥接）。
所以容器网络 ≠ 完全独立的物理网络，
而是宿主机网络上通过虚拟设备隔离出来的“虚拟网络”。


Docker 提供的几种网络模式
| 模式                        | 容器与宿主机网络关系                    | 特点                      | 常见用途             |
| ------------------------- | ----------------------------- | ----------------------- | ---------------- |
| **bridge（默认）**            | 容器在一个独立虚拟网桥中，和宿主机隔离，通过 NAT 出网 | 容器有自己的 IP，访问外部时通过宿主机转发  | 一般应用容器（如 Web 服务） |
| **host**                  | 容器和宿主机共享同一个网络命名空间             | 容器直接使用宿主机的 IP，性能好但无隔离   | 性能敏感、网络透明需求      |
| **none**                  | 容器没有网络接口                      | 完全隔离                    | 自定义网络栈的特殊情况      |
| **container:**`<name/id>` | 与另一个容器共享网络命名空间                | 多容器共享网络栈                | Sidecar 模式       |
| **macvlan**               | 容器直接在宿主机物理网卡上创建虚拟子接口          | 容器拥有独立 MAC/IP，可直接被局域网访问 | 高级网络场景，如与物理设备通信  |


+-------------------------------------------------------------+
|                       宿主机 (Host)                         |
|                                                             |
|  +--------------------+       +--------------------------+  |
|  | eth0 (宿主机网卡)  |-------| 外部网络 (Internet/LAN) |  |
|  +--------------------+       +--------------------------+  |
|           |                                               |
|           v                                               |
|     +-----------+                                         |
|     | docker0   |  ← 虚拟网桥 (bridge)                   |
|     +-----------+                                         |
|      |       |                                            |
|   veth0a   veth1a    ← veth pair 虚拟网线                 |
|      |       |                                            |
|  +--------+  +--------+                                   |
|  | 容器 A |  | 容器 B |                                   |
|  | eth0→veth0b | eth0→veth1b |                            |
|  +--------+  +--------+                                   |
|  容器IP:172.17.0.2  容器IP:172.17.0.3                    |
+-------------------------------------------------------------+

每个容器里看到的 eth0 是虚拟网卡；
宿主机会为每个容器创建一对 “veth（虚拟以太网对）”；
一头在容器里，一头连到宿主机的虚拟网桥（docker0）；
宿主机通过 NAT 转发容器流量出外网。


容器与宿主机网络的交互总结
| 方向                | 是否可达                  | 说明 |
| ----------------- | --------------------- | -- |
| 容器 → 外网           | ✅ 默认可达（NAT）           |    |
| 外网 → 容器           | 🚫 默认不行，需要 `-p` 端口映射  |    |
| 容器 ↔ 容器（同 bridge） | ✅ 可直接通信               |    |
| 容器 ↔ 容器（不同网络）     | 🚫 默认不通，需要自定义网络或 link |    |
| 容器 ↔ 宿主机          | ✅ 可通过宿主机 IP 通信        |    |



举个实际例子
假设你在宿主机上执行：
$ docker run -d --name test busybox sleep 9999
$ docker exec test ip addr
你可能看到容器里有个 IP：
eth0: 172.17.0.2
而宿主机上执行：
$ ip addr show docker0
会看到：
docker0: inet 172.17.0.1/16
说明容器的网络是通过虚拟网桥 docker0 和宿主机连接的。
容器访问外网时，会通过 NAT：
172.17.0.2:8080 → [宿主机IP]:随机端口 → Internet


这个 NAT 是由 Linux 内核自带的网络功能（iptables / nftables） 实现的
在 Linux 上，NAT 是由内核模块 netfilter 实现的。
iptables（或新版 nftables）只是它的配置工具。
Docker 并不会自己转发数据包，它只是：
在启动容器时，用 iptables 帮你设置好几条转发和 NAT 规则。
然后，实际的网络包处理完全由 Linux 内核自动完成。

容器 NAT 的工作原理（以默认 bridge 模式为例）:
当容器启动时，Docker 做了三件事：
创建一个虚拟网桥（docker0）
给容器分配一个虚拟 IP（比如 172.17.0.2）
用 iptables 在系统里添加 NAT 转换规则
你可以在宿主机上看到这些规则：
$ sudo iptables -t nat -L -n
输出大致包含：
Chain POSTROUTING (policy ACCEPT)
MASQUERADE  all  --  172.17.0.0/16  !172.17.0.0/16  /* docker */  MASQUERADE
这行规则表示：
当源地址是 172.17.0.0/16（即容器网段）的包要发往外部网络时，
用宿主机的 IP 伪装（MASQUERADE）它。

NAT 转换过程解析:
假设容器的请求是：
源IP: 172.17.0.2:8080 → 目标IP: 8.8.8.8:53
Linux 内核做的事是：
| 阶段                      | 操作                                                      |
| ----------------------- | ------------------------------------------------------- |
| **1. 出站时（POSTROUTING）** | 把源 IP 改成宿主机的公网 IP（例如 192.168.1.10），端口改成随机端口（例如 55000）   |
| **2. 返回时（PREROUTING）**  | 收到返回包后，再根据连接跟踪表（conntrack）把目的 IP 改回原容器的 172.17.0.2:8080 |
整个过程是由 Linux 内核 NAT 模块自动完成 的

为什么 Docker 要做 NAT:
因为容器的 IP（例如 172.17.0.2）属于一个私有网段：
外部网络（比如 Internet）无法直接路由到它；
所以必须用 NAT，把容器的私网 IP 转换成宿主机的公网 IP。
这就是“容器访问外网通过宿主机 NAT”的由来。

Docker 只是帮你“配置好” NAT
Docker 本身不会处理包转发逻辑，它只是做：
iptables -t nat -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
以及打开 Linux 内核的转发开关：
sysctl -w net.ipv4.ip_forward=1
之后，一切转发和 NAT 动作都由内核负责。


当虚拟机要上网或访问局域网里的设备时，有三种典型的方式：
| 模式                | 虚拟机 IP                       | 外界能否访问虚拟机 | 原理简述                   |
| ----------------- | ---------------------------- | --------- | ---------------------- |
| **NAT 模式**        | 由宿主机分配私有 IP（例如 10.0.2.x）     | ❌ 不能直接访问  | 虚拟机通过宿主机的网络做 NAT 转发出网  |
| **Host-only 模式**  | 仅在宿主机和虚拟机之间通信                | ❌         | 形成一个私有网段，不连接外网         |
| **桥接模式（Bridged）** | 与宿主机处于同一个局域网（例如 192.168.1.x） | ✅ 可以直接访问  | 虚拟机“共享”宿主机的物理网卡参与局域网通信 |


| 名称                                       | 属于谁                       | 行为                   | 容器是否和宿主机在同一局域网 |
| ---------------------------------------- | ------------------------- | -------------------- | -------------- |
| **Docker bridge 模式**（默认）                 | Docker 自建的虚拟网桥（`docker0`） | 容器在私有虚拟网段中，通过 NAT 出网 | ❌ 否            |
| **Linux 桥接到物理网卡（macvlan / host bridge）** | 手动配置或 macvlan 网络          | 容器直接连入宿主机所在局域网       | ✅ 是            |
换句话说：
Docker 默认创建的 bridge 网络只是内部虚拟网桥，不是“物理网桥”。

因为在 Linux 术语中，“bridge” 指的是一个虚拟交换机（virtual bridge）
Docker 用它来让多个容器能互通，并不是物理桥接。
也就是说：
这个“bridge”只是容器内部网络之间的“交换机”，
并不把容器直接接到宿主机物理网卡上。

==================================================虚拟机的网桥 & 容器的默认 bridge（docker0）
虚拟机的网桥:
在虚拟化环境（比如 VirtualBox、VMware、KVM）中：
**网桥（Bridge）**通常指把虚拟机的虚拟网卡“桥接”到宿主机的物理网卡上。
结果：虚拟机像局域网中的一台独立主机，能直接和宿主机、局域网的其他设备通信。
虚拟机的网桥 = 宿主机物理网络上的“交换机接口”，VM 就像插到交换机上的一台电脑。

容器的默认 bridge（docker0）:
Docker bridge（docker0）只是一个Linux 内核虚拟交换机：
容器在 docker0 上分配私有 IP（172.17.x.x）；
容器之间可以互相通信；
容器访问外网通过宿主机 NAT 转发。
特点：
容器 IP 在私有网段，外网不可直接访问；
容器和宿主机不是同一局域网；
主要作用是“容器内部通信和 NAT 出网”。
Docker 的 bridge = 宿主机内部的“小虚拟交换机”，只在宿主机内部网络通用。


| 特性          | 虚拟机网桥           | 容器默认 bridge           |
| ----------- | --------------- | --------------------- |
| 是否直接连物理网卡   | ✅ 是             | ❌ 否                   |
| IP 属于宿主机局域网 | ✅ 是             | ❌ 容器私有网段              |
| 外网访问        | 直接              | 通过 NAT                |
| 用途          | VM 外部访问/局域网透明   | 容器内部通信 + NAT 出网       |
| 实现          | 宿主机虚拟交换机 + 网桥接口 | Linux 内核虚拟网桥（docker0） |


==================================================“网桥”
网桥（Network Bridge） 是一种工作在 数据链路层（OSI 第二层） 的网络设备，用来连接两个或多个局域网（LAN），使它们看起来像一个单一的局域网。
它根据 MAC 地址（物理地址） 来转发数据帧；
不关心 IP 地址；
用于分割冲突域（collision domain），但仍处于同一个广播域（broadcast domain）中。


想象两个教室（两个局域网），每个教室里的人都可以互相直接说话。
如果两个教室之间装了一道“网桥”，那么：
教室 A 的人说话时，网桥会“听”到；
网桥会判断，这句话是不是要传到教室 B；
如果是，就转发；
如果不是，就不理会。

这就避免了不必要的广播干扰，但又允许需要通信的部分互通。


| 对比项   | 网桥（Bridge） | 交换机（Switch） | 路由器（Router）      |
| ----- | ---------- | ----------- | ---------------- |
| 工作层   | 数据链路层（第二层） | 数据链路层（第二层）  | 网络层（第三层）         |
| 依据    | MAC 地址     | MAC 地址      | IP 地址            |
| 分割冲突域 | ✅          | ✅           | ✅                |
| 分割广播域 | ❌          | ❌           | ✅                |
| 速度    | 较慢（软件）     | 较快（硬件）      | 较快（硬件+软件）        |
| 功能范围  | 连接局域网      | 网桥的多端口版本    | 连接不同网络（不同 IP 网段） |


Linux/Unix 中的桥接设备（bridge）
你可以通过命令创建一个软件网桥：
ip link add name br0 type bridge
ip link set dev eth0 master br0
ip link set dev eth1 master br0
ip link set dev br0 up
这会把 eth0 和 eth1 两个网卡桥接在一起，让它们像在一个局域网中通信。


虚拟机 / 容器网络中的桥接
当你让虚拟机“桥接网络”时，就是让虚拟机通过宿主机的物理网卡直接参与到局域网中；
Docker 里默认的 docker0 就是一个 Linux bridge。


“让虚拟机通过宿主机的物理网卡直接参与到局域网中”
意思是：
虚拟机在网络上看起来就像是一台真实的物理电脑，
直接接在你的路由器或交换机上。
| 设备        | IP 地址        |
| --------- | ------------ |
| 宿主机（笔记本）  | 192.168.1.10 |
| 打印机       | 192.168.1.30 |
| NAS 服务器   | 192.168.1.40 |
| 虚拟机（桥接模式） | 192.168.1.50 |
在这个模式下：
虚拟机会从**同一个 DHCP 服务器（比如路由器）**获取 IP；
其他设备（例如打印机或 NAS）可以直接访问虚拟机；
宿主机的物理网卡相当于被**“虚拟化成多个端口”**，其中一个端口给宿主机自己用，另一个端口给虚拟机。
这里说的“端口”，指的是网络接口（Network Interface）
当我们说：
“宿主机的物理网卡被虚拟化成多个端口”
意思是：
系统在软件层面上创建了多个虚拟网卡接口；
每个接口都可以拥有自己的 MAC 地址；
它们共享同一块物理网卡的底层传输能力。


这两个“端口”的区别
| 概念               | 所属层              | 含义                             | 举例                         |
| ---------------- | ---------------- | ------------------------------ | -------------------------- |
| **物理/链路层的端口**    | OSI 第 2 层（数据链路层） | 网卡（NIC）的一个“接口”或“插口”，连接到网络中     | 网卡上的接口、虚拟网卡、网桥接口           |
| **传输层的端口（Port）** | OSI 第 4 层（传输层）   | TCP 或 UDP 协议中的逻辑端口，用于区分不同的应用服务 | HTTP 80, HTTPS 443, SSH 22 |



==================================================传输层的端口
传输层的端口（port）确实是通信的“端点”，而不是一个进程本身。
不过它和进程之间有着紧密的关系

它的作用是告诉操作系统：
“这条进入的数据流要交给哪个应用（socket）”。

端口 ≠ 进程
它更准确地说是「通信通道的逻辑标识」
操作系统维护一张表，记录着：
哪个端口号（+协议）对应哪个socket（通信套接字）。
一个 socket 是一个通信端点，由四元组唯一标识：
<源IP, 源端口, 目的IP, 目的端口>
| 字段   | 值                          |
| ---- | -------------------------- |
| 源IP  | 192.168.1.10               |
| 源端口  | 53001                      |
| 目的IP | 142.250.190.14（google.com） |
| 目的端口 | 443（HTTPS）                 |
操作系统通过这四元组知道：
这条 TCP 连接属于哪个应用；
数据包要交给哪个 socket。

端口与进程的关系:
一个进程可以打开一个或多个 socket；
每个 socket 都会绑定一个或多个端口；
因此可以说：
端口是 通信的端点（socket 的标识）；
socket 属于某个 进程；
所以端口间接地对应到某个进程。


正常情况下：一个 socket 对应一个端口
严格来说，一个单独的 socket 不能直接绑定多个端口号。
但是有几种变通或等价的机制，实现“一个逻辑服务监听多个端口”的效果
1. 多个 socket 分别绑定不同端口，共享同一逻辑服务
你的服务进程创建多个 socket；
每个 socket bind() 不同端口；
程序用多路复用（如 select / epoll）监听它们；
在逻辑上相当于“一个程序监听多个端口”。
fd1 = socket(AF_INET, SOCK_STREAM, 0);
bind(fd1, 0.0.0.0:80);

fd2 = socket(AF_INET, SOCK_STREAM, 0);
bind(fd2, 0.0.0.0:8080);

listen(fd1);
listen(fd2);
这就是：
一个进程（服务）
拥有多个 socket
每个 socket 绑定一个端口

2. “端口复用”（SO_REUSEPORT）
如果你看到某个服务（如 Nginx）在同一个端口上有多个监听 socket，那是因为它用了：
setsockopt(fd, SOL_SOCKET, SO_REUSEPORT, ...)
允许多个 socket 绑定同一个端口号；
但每个 socket 依然是“绑定一个端口”；
内核会在这些 socket 之间负载均衡传入连接；
常用于多核服务器提高性能。
所以这其实是“多个 socket 绑定同一个端口”，不是“一个 socket 多个端口”。

3. “通配地址”绑定多个 IP 上的相同端口
一个 socket 可以绑定到通配符地址：
bind(fd, 0.0.0.0:8080)
表示：
所有本机 IP（如 eth0、eth1）的 8080 端口；
都由这个 socket 接收；
但注意，这仍然是 “多个 IP，共享一个端口号”，而不是多个端口。


通配符地址（Wildcard Address）
“不指定具体的本地 IP，而是绑定到本机上所有可用的网络接口（IP 地址）。”
通配符地址就是：
0.0.0.0
在 IPv6 中：
::

一台主机往往有多个网络接口，例如：
| 接口      | IP 地址        | 说明     |
| ------- | ------------ | ------ |
| `eth0`  | 192.168.1.10 | 有线网卡   |
| `wlan0` | 192.168.2.5  | 无线网卡   |
| `lo`    | 127.0.0.1    | 本地回环接口 |

bind(fd, 192.168.1.10:8080)
那么：
这个 socket 只会接收发到 192.168.1.10:8080 的数据；
如果别人访问 192.168.2.5:8080，这个 socket 收不到。

bind(fd, 0.0.0.0:8080)
那么：
所有本机 IP 上的端口 8080 都会被这个 socket 监听；
无论外界访问哪个网卡的 IP，只要端口是 8080，都能接入。
所以 0.0.0.0 表示 “绑定所有本地地址”。


==================================================NAT 转发流程 —— 从虚拟机发包到外网返回
当虚拟机处于 NAT 模式（Network Address Translation）时，
虚拟机网络拓扑大致如下：

[虚拟机] 10.0.2.15
    │
(虚拟网卡 vNIC)
    │
[虚拟 NAT 网关] 10.0.2.2
    │
(宿主机内置 NAT 引擎)
    │
[宿主机物理网卡] 192.168.1.10
    │
[路由器 / 外网]

虚拟机看到的网关是一个虚拟 NAT 设备（由宿主机提供）；
外部世界并不知道虚拟机的存在；
所有数据都“经过宿主机转发”出网。


完整的 NAT 转发流程（出站）
第 1 步：虚拟机发出数据包
虚拟机想访问外网（例如访问 1.1.1.1:80）：
| 字段    | 值              |
| ----- | -------------- |
| 源 IP  | 10.0.2.15（虚拟机） |
| 源端口   | 53001          |
| 目的 IP | 1.1.1.1        |
| 目的端口  | 80             |
它把包交给自己的默认网关 10.0.2.2（其实是宿主机的虚拟 NAT 模块）。

第 2 步：宿主机 NAT 模块接收数据包
宿主机（或虚拟化软件，例如 VMware、VirtualBox、QEMU）中有一个小型 NAT 服务进程，
例如：
VMware 的 vmnat.exe
VirtualBox 的 VBoxNetNAT
QEMU 的 slirp
它相当于一个“虚拟路由器 + NAT 引擎”。
它会做两件事：
（1）修改包头（源地址转换）
源 IP 从 10.0.2.15 改成宿主机的 192.168.1.10
源端口从 53001 改成一个新的随机端口（例如 61234）
（2）在 NAT 表中记录映射
10.0.2.15:53001  <-> 192.168.1.10:61234

第 3 步：宿主机通过物理网卡发包出网
修改后的数据包被发到真实网络上：
| 字段    | 值                 |
| ----- | ----------------- |
| 源 IP  | 192.168.1.10（宿主机） |
| 源端口   | 61234             |
| 目的 IP | 1.1.1.1           |
| 目的端口  | 80                |
在外部网络看来，这个连接是宿主机自己发的。
虚拟机此时完全“隐身”。

第 4 步：外网服务器响应
目标服务器（1.1.1.1）返回数据包：
| 源 IP | 1.1.1.1 |
| 源端口 | 80 |
| 目的 IP | 192.168.1.10 |
| 目的端口 | 61234 |
宿主机接收到包后，会查 NAT 表：
192.168.1.10:61234 → 对应虚拟机 10.0.2.15:53001

第 5 步：宿主机做反向转换
宿主机 NAT 模块执行：
把目标地址改回虚拟机的 IP；
把目标端口改回虚拟机的原始端口；
新的包变为：
| 源 IP | 1.1.1.1 |
| 源端口 | 80 |
| 目的 IP | 10.0.2.15 |
| 目的端口 | 53001 |
然后转发给虚拟机。

第 6 步：虚拟机收到响应
虚拟机收到包后，以为自己直接和 1.1.1.1 通信；
实际上中间的 NAT 引擎在帮它做“地址伪装”。


反方向：外部无法主动访问虚拟机的原因
因为 NAT 的映射是临时的、出站触发的。
当虚拟机主动发起连接时，宿主机会记录 NAT 映射；
外部主机无法直接访问 10.0.2.15（它是内部私网地址）；
宿主机也不会把外部请求“随机映射”到虚拟机；
除非你手动配置 端口转发（port forwarding）。