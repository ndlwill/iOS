https://developer.apple.com/videos/play/tech-talks/10855/


==================================================UI animation hitches and the render loop // UI 动画卡顿和渲染循环
讨论 应用中的滚动和动画卡顿，并深入了解 渲染循环（Render Loop）


1. 什么是卡顿（hitch）:
人们可能会用手指在屏幕上滑动来滚动内容；
也可能点击一个按钮，期待得到反馈；
或在不同视图层级之间切换。

这些动画在用户和屏幕上的内容之间建立起一种视觉上的连接感。
动画卡顿 会导致动画出现跳动，破坏这种连接

#####
卡顿就是指某一帧比预期更晚出现在屏幕上的情况。
#####

例子：滚动一个 collection view。
比如在一个「餐食计划」应用中，用户上滑屏幕时，滚动视图会随着手指移动。
但在滚动过程中，我们注意到内容有一次突然跳动。

如果我们逐帧来看，可以看到在前三帧里手指和内容保持同步，但在接下来的那一帧里，内容似乎停滞了。
这是因为第三帧实际上被重复显示，多停留了一帧。
最后第四帧出现了，看起来像是跳到手指所在的位置。
第三帧之所以被重复，是因为第四帧来晚了，于是用户就看到了卡顿。

#####
卡顿的原因是渲染循环未能按时完成一帧。
#####


2. 渲染循环（Render Loop）
渲染循环是一个连续的过程：触摸事件被传递给应用，然后 UI 的更改被提交到操作系统，在那里最终生成一帧。
#####
这是一个循环，它按照设备的刷新率发生。
#####
在 iPhone 和 iPad 上，这是每秒 60 帧，也就是说每 16.67 毫秒可以显示一帧。
在 iPad Pro 上，这是每秒 120 帧，也就是说每 8.33 毫秒可以显示一帧。

在每一帧的开始，硬件会发出一个称为 VSYNC 的事件。
VSYNC 表示新的一帧必须准备好的时刻。
我们会在显示轨迹中标记它们，这样就能很容易看出截止时间。
#####
VSYNC 指示屏幕何时将帧交换到显示屏上
#####
渲染循环是按照 VSYNC 的节奏来进行的。
它必须在过程中完成一些检查点，才能确保帧准备好。
它被分为三个阶段:
第一个阶段在你的应用里，处理事件并修改 UI。
这些工作必须在下一个 VSYNC 之前完成，才能进入下一个阶段。
下一个阶段发生在一个独立的进程中，叫做渲染服务器（render server）。
UI 的实际渲染就在这里发生。
这个阶段也必须在下一个 VSYNC 前完成，这样帧才能进入第三个也是最后一个阶段——显示出来。


#####
渲染循环分为三个主要阶段：// The render loop
应用阶段：事件处理 + UI 更新 // App
渲染服务阶段（系统进程）：将 UI 渲染成图像 // Render Server
显示阶段：最终呈现在屏幕上 // On the display
#####


关于这个三阶段过程有一个重要的细节：一帧在显示前会被处理两帧时间。
我们称之为双缓冲（double buffering），不过还有另一种模式。

为了避免卡顿，系统可能会切换到三缓冲（triple buffering），在这种模式下，渲染服务器会多得到一帧的时间来完成工作。

fallback mode: 兜底模式, 后备方案
由于三缓冲是兜底模式，在讨论渲染循环中的卡顿时，我们主要关注双缓冲。


总体上，整个渲染循环由五个阶段组成。
1. 循环从第一个阶段开始，即事件阶段（event phase）。
你的应用会处理触摸事件，并决定 UI 是否需要变化。
2. 接下来是提交阶段（commit phase）。
在提交阶段，你的应用更新 UI，并把更新提交给渲染服务器进行渲染。
3. 在下一个 VSYNC 时，渲染服务器会接收提交的内容，并在渲染准备阶段（render prepare phase）为 GPU 绘制做准备。
4. 在渲染执行阶段（render execute phase），GPU 会把 UI 绘制成最终图像
5. 这样在下一个 VSYNC 时，帧就能显示给用户。


尽管渲染发生在单独的进程中，但它是替你的应用工作的，所以你必须确保你的图层树能被及时处理和绘制。


在这个例子里，我们会跟随一帧在渲染循环中的流转，看看它经历的各个阶段:
首先是事件阶段，应用会接收到事件。
这些事件包括触摸、网络回调、键盘输入和定时器。
应用可以通过修改图层层级来响应这些事件。
例如，应用可以修改某个图层的背景色，或者改变它的大小和位置。
但是当应用更新图层的边界时，Core Animation 也会调用 setNeedsLayout。
这会标记所有需要重新计算布局的图层。
系统会合并这些“需要布局”的请求，并在提交阶段按顺序执行它们，以减少重复工作。
如果需要进行布局，那么一旦事件阶段完成，提交阶段就会自动开始。
首先，系统会依次对所有需要布局的图层进行布局，从父图层到子图层。
布局是一个常见的性能瓶颈，所以要注意，你的应用只有几毫秒的时间来完成这些工作。
有些视图还需要自定义绘制，比如标签（label）、图片视图，或者任何重写了 drawRect 的视图。
如果这些视图需要更新显示，就必须调用 setNeedsDisplay。
和布局一样，系统会合并这些绘制请求，并在所有布局完成后再统一执行。
在绘制过程中，每个自定义绘制的图层都会得到一个带有纹理的 Core Graphics 上下文，在其中进行绘制。
从 Core Animation 的角度来看，这些图层现在就只是图像。
#####
现在所有图层的布局和绘制都完成后，整个修改过的图层树会被收集起来，并发送到渲染服务器进行渲染。
#####
接下来进入渲染服务器，它负责把图层树转化为真正可显示的图像。
在准备阶段，渲染服务器会遍历应用的图层树，并准备一个供 GPU 执行的线性管线。
它会从最顶层开始，按照父到子、兄弟之间的顺序处理，从而把图层从后到前排列好。
然后，这个线性管线会交给 GPU，GPU 会把每个图层合成到最终纹理中。
有些图层可能需要更长的渲染时间，这是另一个常见的性能瓶颈

所以一旦 GPU 执行并渲染出图像，它就准备好在下一个 VSYNC 时显示出来了。
渲染循环中的每个阶段都对性能非常敏感，并且都有截止时间。
截止时间就是下一个 VSYNC。
为了达到目标帧率并保持较低的输入延迟，这整个过程实际上在每一帧都是并行进行的。
这样，管线就能并行工作，我们的应用在准备新的一帧时，系统可以同时渲染上一帧。


3. 卡顿的两种类型
(1)提交卡顿（commit hitch）
发生在 app 进程中
事件处理或提交阶段超时，导致下一帧没有可渲染数据
结果：帧延迟至少 16.67 毫秒
(2)渲染卡顿（render hitch）
发生在渲染服务中
渲染准备或执行阶段超时，帧未按时完成
同样会导致 16.67 毫秒的延迟

主要有两种类型：提交卡顿（commit hitch），发生在应用进程中；渲染卡顿（render hitch），发生在渲染服务器中。
(1)提交卡顿是指应用处理事件或提交时花费的时间太长。
提交花了太长时间，错过了截止点，所以在下一个 VSYNC 时，渲染服务器没有可处理的内容，只能等到再下一个 VSYNC 才能开始渲染。
于是我们把帧的交付时间延迟了一帧。
换算成毫秒，就是在 iPhone 或 iPad 上延迟了 16.67 毫秒。
我们把这个延迟时长称为“卡顿时间（hitch time）”，单位是毫秒。
如果提交的工作花费更久，超过了下一个 VSYNC，那么帧就会延迟两帧，也就是 33.34 毫秒。
这意味着用户在这 33.34 毫秒里没有看到流畅的滚动。

查找并修复提交阶段的卡顿:
https://developer.apple.com/videos/play/tech-talks/10856/

(2)第二种卡顿是渲染卡顿
当渲染服务器无法按时准备或执行图层树时，就会发生这种情况。
渲染执行阶段耗时过长，超出了 VSYNC 的边界。
因此这一帧没能按时准备好，绿色的帧比预期晚了一帧才显示。
这又带来了 16 毫秒的卡顿时间。

揭秘并消除渲染阶段的卡顿:
https://developer.apple.com/videos/play/tech-talks/10857/


4. 如何测量卡顿
单帧卡顿可以用 卡顿时间（hitch time） 来衡量。
但在更长的滚动、动画过程中，这个指标不太好比较。

我们使用 卡顿时间比率（hitch time ratio）:
定义：某段时间内的 卡顿总时间 ÷ 总时长
单位：毫秒/秒
表示平均每秒钟中有多少毫秒处于卡顿

参考标准：
0 ms/s → 完美
< 5 ms/s → 良好，几乎无感
5–10 ms/s → 用户能注意到，需要关注
10 ms/s → 严重影响体验，必须优化


卡顿时间（hitch time）
在讨论单个卡顿时它非常有用，但在讨论像滚动、动画或过渡这种长期事件时，它可能变得难以处理。
除非每次滚动或动画的时间完全相同，也就是说帧数完全相同，否则很难进行比较。
iOS 设备并不总是刷新屏幕
如果没有提交发送到渲染服务器，就不会生成新的帧。
这使得在不同测试和设备之间比较卡顿时间更加困难。
因此，我们使用一个叫做“卡顿时间比率（hitch time ratio）”的指标。
#####
卡顿时间比率是某个时间区间内的总卡顿时间除以该区间的持续时间。
#####
它的单位是每秒卡顿毫秒数，也就是表示设备每秒钟卡顿的毫秒量。

To learn about how you can use this metric to measure your own app performance, check out "What's New in MetricKit."
想要了解如何使用这个指标衡量自己应用的性能，请查看《MetricKit 的新功能》。
To see how to track hitch time ratio in your test suite, check out "Eliminate Animation Hitches with XCTest."
想要了解如何在测试套件中追踪卡顿时间比率，请查看《用 XCTest 消除动画卡顿》。


使用卡顿时间比率的例子:
这里有 30 帧，在 iPhone 上大约相当于半秒的工作量。// 0.5s
每一帧都按时完成，用户不会看到卡顿。// 0ms
卡顿时间为零，卡顿时间比率也为零。// hitch time ratio = 0 / 0.5 = 0 ms/s

有些帧在屏幕上停留的时间比其他帧长，一些提交和渲染导致了卡顿。
如果把这些卡顿时间加起来，总计为 100.02 毫秒。
在半秒内，我们得到每秒 200.04 毫秒的卡顿时间比率。// 100.02 / 0.5 = 200.04


==================================================渲染循环的五个阶段:
整个过程可分为五个###细分阶段###:
事件阶段（event phase）
app 接收触摸、键盘、网络、定时器等事件
根据需要修改 layer 树

提交阶段（commit phase）
处理布局（setNeedsLayout → layoutSubviews）
处理绘制（setNeedsDisplay → drawRect）
将更新后的 layer 树提交给渲染服务

渲染准备阶段（render prepare）
渲染服务整理 layer 树，生成 GPU 可执行的线性管线

渲染执行阶段（render execute）
GPU 合成所有 layer → 最终纹理图像

显示阶段
在下一个 VSYNC 将图像显示到屏幕上

这些阶段对性能非常敏感，因为每一帧都有硬性截止时间。


==================================================#####关于这个三阶段过程有一个重要的细节：一帧在显示前会被处理两帧时间。#####
“一帧在显示前会被处理两帧时间” —— 其实是在描述 渲染管线（render pipeline）里的延迟，尤其是 双缓冲（double buffering） 的特性。


单缓冲（single buffering）：只有一个 buffer，CPU/GPU 把画面直接画在屏幕要显示的区域。如果绘制没完成就刷新，就会看到撕裂、闪烁。


/*
双缓冲（double buffering）的核心就是 避免用户看到“正在绘制过程中的不完整画面”。
流程可以这样理解：
前缓冲（front buffer）
当前屏幕上正在显示的完整一帧画面。
在下一帧绘制完成前，它一直保持不变。

后缓冲（back buffer）
GPU/CPU 在这里绘制“下一帧”的所有内容。
用户看不到它，哪怕绘制到一半。

交换（swap）
当后缓冲完全绘制好一帧后，系统执行 buffer swap（交换指针或者翻转）。
原来的后缓冲 → 变成前缓冲（开始显示）。
原来的前缓冲 → 变成后缓冲（供下一帧绘制）。
*/
双缓冲（double buffering）：有两个 buffer ——
前缓冲（front buffer）：正在显示的画面。
后缓冲（back buffer）：正在绘制的下一帧。
当绘制完成后，前后缓冲交换（swap），这样屏幕显示的是完整的一帧。


为什么会有“两帧时间的延迟”？
在 三阶段管线（App → Render Server → GPU/Display） 里，每个阶段是异步的、串联的：
第 0 帧：App 生成绘制指令 → 提交给 Render Server。
第 1 帧：Render Server 把指令打包 → 提交给 GPU。
第 2 帧：GPU 真正把像素画好 → 进入 front buffer → 等待 VSync 才显示到屏幕上。
所以某一帧从 App 逻辑处理开始，到用户肉眼看到，往往已经过了 两个完整的 VSync 间隔。

换句话说：
当你在第 N 帧里调用 drawRect 或 CoreAnimation 提交，
这帧真正出现在屏幕上，通常要等到 第 N+2 帧。

假设屏幕刷新率是 60Hz（每帧 16.67ms）：
t=0ms（第 0 帧）：App 准备渲染数据。
t=16ms（第 1 帧）：Render Server 处理数据。
t=33ms（第 2 帧）：GPU 绘制完成，缓冲区交换。
t=33~50ms：用户终于看到第 0 帧的内容。
所以用户看到的画面，其实是 App 两帧之前的产物。

总结：
“一帧在显示前会被处理两帧时间”
意思是：在双缓冲机制下，App 渲染的一帧画面，并不会立刻显示，而是要经历 App → Render Server → GPU 三个阶段，通常需要经过 两个刷新周期的延迟，用户才会看到这帧画面。


App 处理逻辑 & 准备渲染数据
   ↓
Render Server / CoreAnimation 合成图层
   ↓
GPU 渲染到屏幕缓冲区

三个阶段并不是都在“画图”
App 阶段：准备绘制指令、计算布局、做动画插值。它本身并不直接“画像素”。
Render Server 阶段：收集 App 提交的图层树（layer tree）、做合成指令。也还没开始真正 rasterize（光栅化）。
GPU 阶段：才是真正把纹理/像素画进缓冲区。
所以前两步其实是 准备和排队工作，画面依旧显示的是上一次已经完成的 frame，这一点是完全正常的。


表面看起来 延迟了两个 VSync，好像“卡住了”，但实际上这并不等于 卡顿（hitch/jank）。
延迟 ≠ 卡顿
延迟 (latency)：
每一帧都要经过 App → Render Server → GPU 这三段流水线，所以画面天然会“慢两拍”。这是一种稳定、可预期的 固定延迟。
就像水管里的水：你拧开水龙头，水要经过两米的管子才流出来。水流得很顺，但总是比动作晚两秒。

卡顿 (hitch/jank)：
当某个阶段没有在一个 VSync 周期内完成工作（比如 GPU 绘制超时、App 主线程阻塞），流水线就“断档”了。
就像水管中间突然被堵住一段，水流忽然断了，然后又继续出水，这就是卡顿。


为什么双缓冲下不算“卡顿”？
只要流水线满负荷稳定运转，虽然有两帧延迟，但每帧都在固定间隔（16.67ms）输出，用户看到的就是平滑的动画。
卡顿发生在流水线没能及时交付，导致某个 VSync 没有新画面可显示 → 屏幕只能重复显示旧帧 → 人眼就感受到“卡了一下”。


举个例子
假设 60Hz 屏幕（1 帧 = 16.67ms）：
正常情况（稳定延迟）：
App 提交帧 #1 → 经过两帧 → 用户在 33ms 时看到它
App 提交帧 #2 → 经过两帧 → 用户在 50ms 时看到它
App 提交帧 #3 → 经过两帧 → 用户在 66ms 时看到它
每帧都延迟了，但间隔是均匀的 → 流畅。

卡顿情况（某帧没赶上 VSync）：
App 提交帧 #2 → Render/GPU 超时 → 没有赶上第 50ms
屏幕在 50ms 时只能重复显示帧 #1
下一次更新要等到 66ms
用户看到的是：#1 → #1（重复）→ #3 → … → 卡顿感出现。


移动端渲染管线里（iOS 都类似），第二帧（Frame #2）会被丢弃，而不是延迟显示。
为什么要丢弃？
渲染管线是跟着 VSync 节奏走的：
VSync 到来 → 如果有新帧，显示新帧。
如果没有新帧，就只能 重复显示上一帧。
假设 60Hz（16.67ms 一帧）：
t=33ms：Frame #1 准备好，显示。
t=50ms：Frame #2 还没准备好 → 屏幕只能再显示一次 Frame #1。
t=66ms：Frame #3 已经准备好了 → 屏幕直接显示 Frame #3。
Frame #2 因为错过了 VSync“上车机会”，就算晚点准备好，也不会再显示（因为屏幕只认“下一班车”）。

为什么不延迟显示？
如果让 Frame #2 晚点再显示，会导致：
输出节奏被打乱 → 帧率不稳定，动画会时快时慢。
延迟积累 → 后续所有帧都“排队”，用户看到的画面会越来越滞后。
所以系统的策略是：
严格跟着 VSync 节奏走。
没赶上的帧直接丢掉。
这样保证后续帧依旧能稳定输出，用户体验更一致。


==================================================卡顿（hitch/stutter）的本质
双缓冲 + 垂直同步（vsync）的情况下，流程是这样的：

1. vsync 信号到来（屏幕要刷新一次）：
系统会尝试把后缓冲（back buffer）里已经准备好的完整画面交换到前缓冲（front buffer），送去显示。

2. 如果后缓冲已经绘制完成：
swap 成功 → 屏幕显示最新的一帧。

3. 如果后缓冲还没绘制完：
没办法交换 → 只能继续显示“上一次的前缓冲”，也就是上一帧画面被重复显示一帧时间。
用户就感受到画面“停顿了一下”。
这就是所谓的 掉帧 / 卡顿。


举个例子（假设刷新率 60Hz，一帧时间 16.67ms）：
如果 GPU 在 16.67ms 内画完 → 流畅，60fps。
如果 GPU 花了 25ms 才画完 → 错过了这一轮 vsync → 只能等下一轮 → 实际帧率掉到 30fps，看起来就是一卡顿。


这就是为什么图形学里常说：
保证每帧的绘制时间 ≤ 屏幕刷新间隔，否则就会“卡”。


Core Animation 的缓冲机制
iOS 的 屏幕渲染主要通过 Core Animation（CA）和 UIKit/SwiftUI）完成。
Core Animation 会把界面元素组成 图层树（layer tree），然后交给 Render Server（backing store， 后备存储） 进行合成。
屏幕显示总是从 前缓冲（front buffer） 拿数据，而 GPU 在 后缓冲（back buffer） 绘制下一帧。
所以用户看到的画面永远是已经完成的前缓冲内容，后缓冲正在准备下一帧。


iOS 渲染管线默认是双缓冲：前缓冲显示，后缓冲绘制。



==================================================Find and fix hitches in the commit phase
iOS 使用渲染循环来显示你的视图
用户的触摸事件会发送到你的应用，应用通过修改视图来响应，而这些视图最终会由 iOS 渲染到屏幕上。


例子:
#####
一个应用视图层级的示例，目前它正在等待事件。
当接收到触摸事件后，视图会响应事件，通过改变背景颜色或某些子视图的frame来处理事件。
系统会记录这些子视图在下一次提交事务中需要进行布局（layout）或显示（display）。
在提交事务期间，需要显示或布局的视图将被更新，对应地会调用 drawRect 或 layoutSubviews 方法。
#####

提交事务（commit transaction）
提交事务中涉及的不同阶段:
共有四个步骤：布局阶段（layout phase）、显示阶段（display phase）、准备阶段（prepare phase），最后是提交阶段（commit phase）。
在布局阶段，会对每个需要布局的视图调用 layoutSubviews。
你可以通过改变视图的位置、添加或移除视图，或者显式调用 setNeedsLayout 来标记该视图需要布局。
在显示阶段，会对每个需要显示的视图调用 drawRect。
你可以通过向视图层级中添加重写了 drawRect 的视图，或者显式调用 setNeedsDisplay 来表明该视图需要显示。
在准备阶段，尚未解码的图像将在此步骤中进行解码。
对于大图像，这类操作可能会花费相当长的时间。
此外，如果图像的颜色格式 GPU 无法直接处理，它也会在此步骤中被转换。
这意味着图像需要被复制，而不能直接使用原始指针，从而会增加额外的时间和内存开销。

To learn more about optimizing the images in your app, watch the "Image and Graphics Best Practices" video.
想了解更多关于应用中图像优化的内容，请观看《图像与图形最佳实践》视频。

最后，在提交阶段，视图层级会被递归打包，并发送到渲染服务器。
请注意，深层的视图层级会花费更长的打包时间。


find hitches with Instruments // 使用 Instruments 来查找卡顿
“Hitches” 轨迹显示卡顿及其持续时间。
“User Events” 轨迹显示与卡顿帧对应的用户事件。
“Commits” 轨迹显示提交阶段以及在此阶段提交的进程。
“Frame Lifetimes” 轨迹显示生成卡顿帧的整个耗时。
“Built-in Display” 轨迹显示所有实际显示的帧以及 VSYNC 事件。

#####
每次出队复用 cell 时，我们都会移除之前的所有标签子视图，并重新创建显示所有标签所需的视图。
我们不需要清空 menuItem
当我们设置新 cell 的 menuItem 时，可以利用复用逻辑，###避免昂贵的视图层级操作###
#####

"Using Time Profiler in Instruments" video


避免提交阶段卡顿的建议:
第一点建议是保持视图轻量化
尽量利用 CALayer 提供的 GPU 加速属性，避免使用 CPU 进行自定义绘制。
如果确实需要自定义绘制，一定要测量其性能。
避免任何空的 drawRect 实现，因为系统仍需执行额外工作，这会增加即将到来的提交事务的时间和内存消耗。
尽量复用视图，以避免昂贵的视图层级操作，例如添加或移除视图。
关于隐藏视图，如果在动画中需要停止显示某个视图，尽量使用视图的 hidden 属性，这样开销更小。

第二点建议是减少昂贵和重复的布局操作。
仅在确实需要更新布局时调用 setNeedsLayout。
layoutIfNeeded 会延长当前提交事务的生命周期，可能导致卡顿。
大多数情况下，你可以等待下一个运行循环再更新布局。
尽量使用最少的约束，以避免增加布局求解的复杂度。

最后，一个视图只应使自己或子视图失效，而不要使同级视图或父视图失效。
否则，视图的布局将被递归地再次失效。


watch the two WWDC talks:
performance layout and image and graphics best practices



==================================================Demystify and eliminate hitches in the render phase
iOS 使用渲染循环（Render Loop）来显示你的视图，而“hitch”（卡顿）是指渲染循环未能及时完成一帧以供显示的任何情况。

渲染卡顿，也就是由“渲染准备”（render prepare）和“渲染执行”（render execute）阶段的慢导致的卡顿。

看一些优化图层树的建议，以防止卡顿影响用户体验。
在提交阶段（commit phase），应用会修改其 UI，并提交更新后的 UI 图层树以进行处理。
我们称这些提交为“commit”，渲染服务器负责为所有前台进程渲染这些 commit。
如果渲染服务器的工作耗时超过一帧的时间，就会发生卡顿。
尽管这些工作发生在应用进程之外，但渲染工作是代表你的应用完成的，因此你需要对渲染图层树所需的时间负责。

渲染服务器有两个阶段：渲染准备（render prepare）和渲染执行（render execute）。
render prepare:
breaks down animations from the commit to be renderd over time. // 将提交的动画分解成需要随时间渲染的动画。
breaks dwon layers and effects into a step-by-step plan of simple operations. // 将图层和效果分解成简单操作的分步计划。
render execute:
draws each step in the pipeline using the GPU. // 使用 GPU 绘制管道中的每个步骤。
compiles the final image for display. // 编译最终图像以供显示。

渲染准备阶段是将图层树编译成 GPU 可执行的一系列简单操作的阶段。
跨多帧进行的动画也在这一阶段处理。
在渲染执行阶段，GPU 将应用的图层绘制到最终图像中，准备显示。

示例渲染：
我们从应用提交给渲染服务器的图层树开始
渲染服务器会逐层处理，将图层编译成一系列绘制命令，GPU 可以按照从后到前的顺序绘制 UI。
从根节点开始，渲染服务器会从兄弟节点到兄弟节点、从父节点到子节点，直到遍历完层级中的每个图层。
最终，它得到 GPU 在下一个执行阶段可以执行的完整绘制管线。
GPU 的工作是将这条管线的每个步骤绘制到中心的最终纹理中。
这个纹理将在显示阶段呈现在屏幕上。

从第一个蓝色图层开始，GPU 会在指定边界内绘制颜色。
接着，深蓝色被绘制到其边界内，然后处理下一个图层。
但现在，GPU 需要绘制阴影。
阴影的形状由接下来的两个图层决定，因此 GPU 并不知道该绘制何种形状的阴影。
如果我们先绘制圆形和条形，阴影会把它们覆盖成黑色，看起来会不正确。
这意味着 GPU 遇到了瓶颈，为了继续，它必须切换到另一个纹理来计算阴影形状。
我们称之为“离屏渲染”（offscreen rendering），因为绘制发生在最终纹理之外的其他地方。
在这里，它可以先绘制圆形和条形。
现在阴影形状被隔离到离屏纹理中。
GPU 有了制作阴影所需的一切：先将图层变黑，然后进行模糊处理。
然后它可以将离屏纹理复制到最终纹理中，阴影图层就完成了。
下一步是再次绘制圆形和矩形。
最后，GPU 会将应用绘制的文本图像复制到顶部。

现在我们完成了两个渲染阶段，这一帧已准备好显示。

但为了绘制阴影，我们必须使用一个特殊技巧，这导致渲染耗时更长。
#####
这被称为离屏通道（offscreen pass）。
离屏通道是指 GPU 必须先在其他地方渲染图层，然后再复制到最终纹理的情况。

在阴影示例中，它必须先绘制图层以确定最终形状。
离屏通道会累积并导致渲染卡顿，因此在应用中监控并尽量避免它们非常重要。
// There are four main types of offscreen passes that can be optimized: shadows, masks, rounded rectangles and visual effects.
可以优化的离屏通道主要有四种类型：阴影、遮罩、圆角矩形和视觉效果。
#####

在示例渲染中，我们看到了阴影离屏的例子。
在这种情况下，渲染器没有足够的信息来绘制阴影，必须先绘制附着的图层。

第二种离屏情况是当一个图层或图层树需要使用遮罩（masking）。
渲染器需要渲染被遮罩的子树。
同时，它还需要避免覆盖遮罩形状之外的像素。
因此，它会先在离屏上渲染整个子树，然后只将遮罩形状内的像素复制回最终纹理。
这种离屏渲染可能导致绘制许多用户永远看不到的像素。

第三种类型与遮罩相关。
图层的圆角有时也需要离屏渲染。
如果信息不足，渲染器可能需要在离屏上绘制整个视图，然后再将圆角形状内的像素复制回最终纹理。

第四种类型来自视觉效果视图（visual effect views）。
UIKit 提供两种视觉效果类型：鲜明效果（vibrancy）和模糊效果（blurring）。
为了应用这些效果，渲染器必须通过离屏通道将视觉效果视图下的内容复制到另一个纹理中。
然后它将视觉效果应用到结果中，并复制回最终纹理。
你会在 UI 导航栏、UI 标签栏以及许多其他标准控件中看到这种情况，因为它在 iOS、tvOS 和 macOS 中非常常见。

因此，这四种离屏类型会降低渲染速度并导致渲染卡顿。


使用 Instruments 查找卡顿:
“用户事件（User Events）”轨道显示与卡顿帧相关的用户事件。
“Commits”轨道显示该帧期间发送到渲染服务器的所有提交阶段。
“Renders”和“GPU”轨道显示渲染服务器执行的工作。
“Frame Lifetimes”轨道显示从事件发生到帧显示所需的整个时间。
最后，“内置显示（Built-In Display）”轨道显示所有显示在屏幕上的帧及其过程中发生的 VSYNC。

你可以将帧生命周期与卡顿持续时间的起点进行比较，以可视化帧本应完成的时间间隔。
在这次卡顿中，我们超出了两帧，从 VSYNC 轨迹来看，提交阶段和渲染阶段都超时了。
这个时间间隔称为“可接受延迟（acceptable latency）”，之后的时间则是卡顿持续时间。
在轨道下方，当选择“卡顿”轨道时，我们可以看到卡顿的详细指标。
我们可以看到卡顿时间和可接受延迟。
这是我们完成帧所需的时间。

// The buffer count is the number of buffers used by the render server at the time of the hitch.
缓冲区数量表示卡顿发生时渲染服务器使用的缓冲区数量。
默认值是 2，但当渲染帧延迟、渲染服务器尝试赶上时，可以是 3。
在双缓冲模式下，我们有两帧，在 iPhone 上大约为 33.34 毫秒，这也是延迟列显示的数值。

还有卡顿类型。
看到既有“高开销的提交（expensive commits）”，也有“高开销的 GPU 时间”，这在上方轨道中已经体现。

我们可以专注于“Render”和“GPU”轨道，并选择它们查看分析器，获取更多关于渲染准备和执行阶段的信息。
一个关键列是“渲染次数（render count）”，可以看到 GPU 必须执行的离屏通道数量。
由于我们知道出现了渲染卡顿，需要查看这些离屏通道，了解它们的原因以及如何修复。
// The best way to look at our layer tree is using the Xcode view debunker.
查看图层树的最佳方法是使用 Xcode 视图调试器。


进入了 View Debugger：
// On the left, we see our view controllers, windows, constraints and views.
在左边，我们可以看到视图控制器、窗口、约束和视图。
从 Xcode 11.2 开始，我们还可以显示图层（layers）。
点击菜单栏的 Editor，然后选择新的 “Show Layers” 项目。
现在在左侧的导航器中，我们可以点击任意一个视图，就能看到它的图层，以及所有的子图层。
当我们选中一个图层时，会出现全新的 Layer Inspector（图层检查器），它会展示图层的一些有用属性。
更重要的是，我们可以看到 offscreen count（离屏渲染计数）。这表示渲染该图层时触发了多少次离屏渲染。
在它下面，是所谓的 offscreen flags（离屏标志）。它们描述了导致离屏渲染的原因。
为了帮助开发者在视图和图层层级中发现并获得优化建议，我们在 Xcode 12 中新增了一类 运行时问题（runtime issue），叫做 Optimization Opportunities（优化机会）。
它默认是启用的，你也可以在 Editor 菜单下找到 “Show Optimization Opportunities” 选项。
在导航器中，我们可以看到某些图层上有紫色的运行时问题指示标记。
Runtime Issue Navigator 查看更详细的信息

我们可以直接把 cornerRadius 设为 10，然后把 masksToBounds 设为 True。
这样只消除了一个离屏
因为我们有子图层，masksToBounds 会强制进行一次离屏渲染，以确保视图被正确裁剪。
但在我们的视图中，我们已经保证子图层不会超出 tag 视图的边界。
所以实际上我们根本不需要遮罩。
我们把 masksToBounds 的调用也删除掉。
现在我们就消除了两个离屏渲染。

“squircle”（方圆形）
从 iOS 13 开始，我们可以使用 cornerCurve 属性，把 cornerRadius 的效果变成 squircle 形状。
只需设置 cornerRadius 并将 cornerCurve 设为 "continuous"。


建议：
#####
当你设置阴影时，一定要指定 shadowPath，这样可以避免大量离屏渲染。
当你需要圆角矩形时，请使用 cornerRadius 和 cornerCurve 属性。
#####
使用 masksToBounds 来裁剪矩形、圆角矩形或椭圆，比自定义遮罩图层性能要高得多。
总的来说，要确保遮罩确实是必要的。如果子树的内容不会超出边界，那就完全关闭 masksToBounds。


==================================================GPU 渲染 + 多层 layer 的绘制
多层 layer 的绘制，关系到 GPU 渲染管线里的 layer compositing（图层合成） 机制. // GPU 按 layer 顺序，一个个绘制到目标 buffer，边画边做 blend。

一般情况下，GPU 是一层层绘制到 offscreen buffer 或者直接到 framebuffer，然后由合成器 (Compositor) 按需合并得到最终的像素，而不是先“合并所有图层”再一次性绘制。

图层的来源:
在 iOS 等系统里，应用里一个个 view/layer 最终会变成 渲染树 (render tree)，每个 layer 通常有：
背景颜色
内容（图片、文字、绘制结果）
变换（transform）
alpha、混合模式


GPU 的处理流程:
每个 layer 都会对应一个绘制命令（draw call）
GPU 按顺序（z-order）把这些 layer 绘制到目标表面（可能是 offscreen buffer，也可能直接是屏幕 backbuffer）。
如果某个 layer 开启了 离屏渲染（比如圆角 + 阴影、遮罩、复杂混合），它会先被绘制到一个临时的 offscreen texture（offscreen buffer），再在合成阶段贴到主 framebuffer 上。
没有触发离屏渲染的 layer，会直接在最终 framebuffer 中 逐层绘制 + blend。


合成 (Compositing):
最终的“合并”不是在 CPU 上先计算出一张合成后的大图，而是：
在 GPU 的 framebuffer 中，像素级地边绘制边混合。
比如有 10 个半透明 layer，GPU 会依次画到 framebuffer，前一个的像素和后一个的像素通过 blend equation 混合。


==================================================渲染管线（rendering pipeline）
渲染管线，可以把它想成是一个 工厂流水线，输入是数据（几何信息、纹理、图层信息），输出是屏幕上的像素。

渲染管线（Rendering Pipeline）就是 GPU 把 场景描述 → 最终图像 的一系列步骤。
它保证 GPU 可以：
批量并行处理数据
逐阶段转换表示（顶点 → 图元 → 像素）
最后得到屏幕帧缓冲里的像素


以最常见的 GPU 图形渲染管线（OpenGL/Metal/Vulkan 的概念模型） 来说：
（1）应用阶段 (Application stage, CPU)
由 CPU 驱动，组织数据，提交 draw call（比如“画一个矩形 layer”）。
CPU 会把顶点、纹理、uniform（参数）送到 GPU。

（2）几何阶段 (Geometry stage, GPU)
包括：
Vertex Shader：处理每个顶点的位置（比如做 transform、旋转、缩放）。
Primitive Assembly（图元装配）：把顶点拼成三角形。
Clipping / Culling：裁掉屏幕外或背面的三角形。

（3）光栅化阶段 (Rasterization stage)
把三角形 离散化成像素片段 (fragment)，决定哪些屏幕像素被覆盖。

（4）像素处理阶段 (Fragment stage)
Fragment Shader：算每个像素的颜色（纹理采样、光照、渐变、混合等）。
Blending：把这个像素和 framebuffer 里已有的像素合成（叠加透明度、颜色混合）。 // #####Fragment blending 是处理 同一个 layer 内 fragment 重叠或透明度叠加。#####


（5）输出合成阶段 (Output Merger / Compositor)
###：GPU 把多个 layer 的 framebuffer 按顺序混合，得到最终的 back buffer。 // #####Output Merger / Compositor 是处理 不同 layer 之间的叠加#####


第 4 步的 Blending 和 第 5 步的 ### 的区别：
涉及到 GPU 渲染管线里 “像素级 blend” 和 “多图层合成” 的区别
第4步：Fragment Stage + Blending
作用范围：仅限于 当前 draw call / 当前 layer 的 fragment/pixel。
做什么：
Fragment Shader 计算每个像素的颜色。
Blending：把这个像素和同一个 layer/framebuffer 内已有的像素混合（比如半透明、alpha blend）。
注意：这时处理的是 单个 layer 内部的像素融合，或者单个物体上的像素混合，不涉及其他 layer。
第5步：Output Merger / Compositor
作用范围：跨 多个 layer。
做什么：
GPU 或系统把不同 layer（比如 UIView 层级、CALayer、SpriteLayer）的 framebuffer 按顺序叠加。
这是 多层合成，得到最终的 back buffer（显示给屏幕）。
原因：
iOS/Metal/DirectX/OpenGL 都支持 多 layer 的渲染。每个 layer 自己可能是半透明的。
你在 fragment shader 内 blending 只处理了 layer 内部像素，但屏幕上最终显示的颜色还需要考虑 层级顺序，比如上层 layer 叠加下层 layer。

| 阶段                         | 作用范围      | blend / 合成内容                                              |
| -------------------------- | --------- | --------------------------------------------------------- |
| Fragment Stage + Blending  | 单 layer 内 | 每个 pixel 与 layer 内已有 pixel 混合                             |
| Output Merger / Compositor | 多 layer   | 各 layer framebuffer 按 z-order / alpha 混合，得到最终 back buffer |


渲染管线的理解方法
你可以这样理解：
数据流：顶点数据 → 图元 → 片段 → 像素
加工厂流水线：每个阶段都只做一件事（变换 / 光栅化 / 着色 / 混合）
并行性：每个顶点、每个像素都可以在成千上万个 GPU core 上并行处理。


==================================================primitive
GPU 接收到 顶点（vertex） 数据后，需要把它们 组合成基本图形（primitive），比如：
三角形（Triangle）
线段（Line）
点（Point）
这个过程就是 Primitive Assembly。
为光栅化做准备


==================================================（4）像素处理阶段 (Fragment stage)的 Blending
Fragment Stage 也是按“绘制顺序”一层层处理的，不过这里的“层”指的是 绘制命令（draw call）或三角形片段的覆盖顺序，而不是 UIView / CALayer 的层级。

假设你有一个 layer，上面画了两个重叠的三角形：
1. 顶点处理（Vertex Shader）
每个三角形的顶点经过顶点变换，计算屏幕空间位置。
2. Primitive Assembly + Rasterization
GPU 将三角形拆成 片段（Fragments），每个片段对应 framebuffer 的像素位置。
如果两个三角形重叠，重叠区域会生成两个 fragment，顺序取决于 draw call 顺序。
3. Fragment Shader
对每个 fragment 计算颜色、纹理采样、光照等。
输出颜色到 framebuffer。
4. Blending（在 Fragment Stage）
GPU 会把这个 fragment 输出的颜色 和 framebuffer 中该像素已有的颜色混合。
例如 alpha blend：outColor = srcColor * srcAlpha + dstColor * (1 - srcAlpha)
注意：这里的 dstColor 就是 framebuffer 当前像素值，也就是前面绘制的 fragment 留下的颜色。

GPU 是 顺序处理每个 fragment，每个 fragment 都会对 framebuffer 的对应像素做 读-计算-写。
所以对于重叠区域，framebuffer 的像素会 被多次更新，每次更新都会做 blending。



“一个位置只有一个像素” → 对的，但 这个像素可以被多个 fragment 访问并多次更新。
假设你在渲染一层（一个 layer）：
Layer 的 framebuffer 是一个二维像素数组（width × height）。
同一个像素位置可能被绘制多次，原因：
（1）多次 draw call / 多个三角形覆盖同一个像素
比如你在这个 layer 上画了两个半透明矩形，它们部分重叠。重叠区域的像素在 framebuffer 上会被 多次 Fragment Shader 处理。
（2）透明度叠加
如果你绘制一个半透明的三角形，它会和 framebuffer 中已有的像素混合，而不是直接覆盖。
所以，虽然物理上这个位置只有一个像素，但 这个像素可能会被多次写入。每次写入都要做 Blending。
例子：
假设 framebuffer 上某个像素初始颜色是 (0, 0, 0, 0)（透明黑），现在有两个半透明红色矩形依次绘制：
第一个矩形 fragment 输出 (1, 0, 0, 0.5)，blend 后这个像素变成 (0.5, 0, 0, 0.5)。
第二个矩形 fragment 输出 (1, 0, 0, 0.5)，blend 后这个像素变成 (0.75, 0, 0, 0.75)。
这个像素位置虽然是 同一个物理位置，但是 被多个 fragment 多次处理，每次都做 blend。


==================================================绘制命令（draw call）
Draw call 的粒度 不是单个三角形，而是 一次 GPU 渲染指令（一次提交），它可以包含 一个或多个三角形

Draw call 是 CPU 告诉 GPU “去画某些顶点/图元（geometry）”的命令。
它包含的信息：
顶点数据（Vertex Buffer）
图元类型（三角形、线段、点）
Shader 程序
材质 / 纹理 / uniform
渲染状态（是否启用 blend、depth test 等）
一个 draw call 可以画：
一个三角形
数百个三角形
整个 mesh（模型）

所以 一个三角形 ≠ 一个 draw call，很多三角形通常会在一次 draw call 中批量提交，减少 CPU-GPU 交互开销。

Fragment stage 的处理顺序：
GPU 会 按 draw call 的顺序处理三角形。
每个三角形经过 rasterization 后生成 fragment。
Fragment 可能覆盖同一个 framebuffer 像素，这时就会发生 多次 blending。
如果你有多个 draw call（比如不同的 mesh、不同的材质），GPU 仍然按 draw call 顺序执行，每个 fragment 都会依次处理。


圆的 GPU 表示：
GPU 只能直接画 点、线、三角形。圆需要被 离散化：
1. 用三角形扇（Triangle Fan）表示圆
圆心 + 圆周上的 N 个点构成 N 个三角形。
比如用 32 个三角形拼一个圆。
2. 用 Fragment Shader 画圆
顶点可能只是一块矩形（quad）。
Fragment shader 里计算 distance(center, fragPos) < radius 来判断像素是否在圆内。
这样也只需要 1 个 draw call（画一个 quad）。

ShaderToy 其实是一个 在线写 shader 的网站/社区 https://www.shadertoy.com/
在线环境：不需要自己搭 OpenGL/WebGL 环境，打开浏览器就能写 GLSL 代码（主要是 Fragment Shader）。

uv 就是 (u, v) 的合称，用来表示 2D 纹理坐标，范围通常是 [0,1]
u → 横向（对应纹理宽度方向）
v → 纵向（对应纹理高度方向）
空间坐标：(x, y, z)
纹理坐标：(u, v, [w])


https://iquilezles.org/
著名的 shader 数学库，有大量 SDF（圆、盒子、心形、星星、螺旋等）


Draw Call 的数量：
方法 1：三角形扇
如果你把圆的所有三角形一次性提交 → 1 个 draw call。
如果你每个三角形单独提交 → 每个三角形 1 个 draw call（不常用，效率低）。
方法 2：Shader 画圆
只画一个 quad → 1 个 draw call，Fragment Shader 决定哪些像素可见。


减少 draw call 一般能提升性能，但要理解背后的原因和限制：
为什么 draw call 多会影响性能：
1. CPU → GPU 通信开销
每次 draw call，CPU 都要告诉 GPU “画这些顶点、用哪个 shader、绑定哪些纹理”等状态。
这个提交过程有 CPU 开销和 GPU 状态切换开销。
2. GPU 状态切换
每次 draw call 可能需要切换 shader、纹理、混合模式等，GPU 内部要做额外工作。
3. #####Fragment 数量不影响 draw call 性能#####
一个 draw call 里即使有上千个三角形，GPU 可以高效并行处理，但 CPU 仍只发出一次命令。
所以 draw call 太多 → CPU 受限 → FPS 下降。

优化策略
批量绘制（Batching）：
尽量把多个小图形/三角形一次 draw call 提交。
合并纹理（Texture Atlas）：
避免每个 draw call 切换纹理。
避免频繁切换 shader / blend / depth state：

注意
不是 draw call 少就一定快
GPU 本身的计算量也很重要（fragment shader 太复杂、屏幕像素太多也会卡）。
平衡 CPU 与 GPU
现代 iOS 游戏或 UIKit 性能优化中，draw call 通常是 CPU 端瓶颈，而 fragment shader 才是 GPU 端瓶颈。

#####
减少 draw call 可以降低 CPU 开销，提高帧率，但同时也要注意 GPU 渲染负载。
#####


==================================================绘制顺序
绘制顺序指的就是底层先绘制，上层后绘制

绘制顺序的定义：
GPU 按 draw call 的提交顺序执行渲染。
先提交的 draw call → 先绘制 → framebuffer 先被写入。
后提交的 draw call → 后绘制 → 覆盖或混合到前面的像素（如果启用了 alpha blend，会混合，而不是完全覆盖）。

和层级的关系（Layer / UIView）
UIKit / Core Animation 会根据 layer 的 z-order / sublayerOrder 来生成 draw call 顺序。
比如：
下层 layer 先生成 draw call → 绘制到底层 framebuffer。
上层 layer 后生成 draw call → 绘制到 framebuffer 上，可能覆盖或 blend 下层内容

在一个 layer 内部的绘制顺序：
单 layer 内的 draw call 顺序也会影响最终像素：
   先画的三角形或图元 → 先写入 framebuffer。
   后画的三角形或图元 → 叠加/混合到前面的像素。
透明图元（alpha < 1）尤其依赖顺序，否则颜色可能不对。


#####
绘制顺序 = draw call 提交顺序。
对 framebuffer 里的同一像素位置：
   先绘制的像素先写入。
   后绘制的像素再 blend/覆盖。
层级越低 → draw call 越先提交 → 先绘制；层级越高 → draw call 越后提交 → 后绘制。
#####


==================================================离屏渲染
如果触发离屏渲染，会在中间阶段先写到一个临时 framebuffer，再作为纹理输入下一次 draw call


为什么圆角会触发离屏渲染？
圆角的本质
当你给一个 layer 设置 cornerRadius，通常你还会配合 masksToBounds = true。
这意味着：
GPU 需要把子内容裁剪成一个 非矩形的区域（圆角矩形）。
普通矩形很好做，因为 framebuffer 本身就是矩形；
但一旦有圆角裁剪，就不是一个简单的矩形填充，而是要 mask 裁剪。

为什么要离屏？
GPU 的 blending/绘制是在 framebuffer 上逐像素进行的。
对于圆角，有两个难点：
1. 不能直接在主 framebuffer 上裁剪
如果直接画在主 framebuffer 上，你没法在“绘制之前”知道哪些像素需要裁剪掉。
结果可能是先画了内容，再强行去掉边角，导致覆盖/混合关系混乱。
2. 需要一个遮罩（mask）操作
圆角本质是“拿一个圆角矩形 mask，把子图层内容做 AND 运算”。
GPU 很擅长做矩形的批量填充，但 mask 裁剪意味着每个像素都要额外计算“在不在圆角内”。

因此系统的做法是：
先离屏渲染 (offscreen rendering)：把该 layer 先画到一个临时的 offscreen buffer（纹理）。
应用圆角 mask：在这个纹理上做裁剪操作（像素级）。
再把结果贴回主 framebuffer。

这样就不会破坏其他 layer 的绘制关系。

性能影响
离屏渲染的代价在于：
多了一次 offscreen buffer 分配 & fill
多了一次纹理贴回主 framebuffer
可能导致 GPU 帧带宽增加（读写显存更频繁）
所以在性能敏感的界面（比如滚动列表 cell）里，如果每个 cell 都是圆角 + mask，就会掉帧。


==================================================GPU 做 阴影（shadow rendering） 的经典方式
为什么要“先变黑”？
阴影的本质：
阴影 ≠ 物体本身，而是物体在光照下挡住光后投射的暗区域。
所以我们不需要保留原图的颜色或细节，只需要一个 不透明的轮廓 来代表“遮挡光的区域”。

变黑（或者更广义：变成统一的暗色、通常带透明度）：
这一步就是把图层的 alpha（形状信息）提取出来，用黑色填充。
相当于“拿到物体的剪影（silhouette）”。


为什么要模糊？
真正的光学阴影通常不是一刀切的锐利边界（除非是理想点光源），而是边缘逐渐扩散。
模糊可以模拟“半影效应”，让阴影看起来自然柔和。


所以整个流程是：
提取图层的形状（alpha mask）：
→ 填充为黑色（带透明度）。
模糊处理：
→ 模拟阴影扩散。
偏移 + 绘制在原图下方：
→ 就成了最终看到的投影。


#####
对于上面的填充为黑色（带透明度）：

之所以 不是纯黑不透明，而是 黑色 + 透明度（alpha），主要有几个原因：
1. 控制阴影的“深浅感”
如果完全不透明的黑色阴影，画出来就像一块黑布，非常生硬。
实际世界里的阴影并不会 100% 遮光，而是 部分透光（环境光会让阴影区域依然有亮度）。
所以在渲染里，通常用 半透明黑色（例如 rgba(0,0,0,0.5)）来表示。这样阴影看起来才自然。

2. 叠加时不会“遮盖一切”
UI 或游戏场景中，阴影是叠加在背景上的。
如果阴影是纯黑 + 不透明，就会完全挡住背景，看不到后面的元素。
用半透明可以让背景透出来，视觉上符合“暗一些，但还能看清”。

3. 模糊时需要透明度过渡
阴影的边缘靠模糊来做“渐隐”。
模糊的原理就是：边缘像素的 alpha 值逐渐过渡到 0。
如果阴影完全不透明，就没法产生平滑过渡，而是生硬的边界。

阴影并不是黑色贴纸，而是“一个透明度渐变的暗层”。
#####


==================================================mask（遮罩）和混合（blending）
mask（遮罩）和混合（blending）有点像，但它们不是一回事

mask 和 blend 都是在 片段着色阶段（fragment stage） 决定每个像素的最终透明度/颜色。
结果都是「原内容 × 某个透明度/颜色系数」。

mask（遮罩）：
表达的是「一张 alpha 图决定另一张图的可见性」。
计算逻辑可以理解成：
finalAlpha = sourceAlpha × maskAlpha
finalColor = sourceColor × maskAlpha
maskLayer 只提供 透明度信息（alpha 通道）。
用途：裁剪图形、任意形状头像、圆形进度条等。

blend（混合）：
表达的是「源像素 和 目标像素 如何合成」。
GPU blending 方程（简化版）：
finalColor = (srcColor × srcFactor) + (dstColor × dstFactor)
例如常见的 alpha blend：
finalColor = srcColor × srcAlpha + dstColor × (1 - srcAlpha)
特点：
blend 不需要额外的 mask 层，直接在 framebuffer 合成。
用途：半透明叠加、阴影、发光、玻璃效果等。


==================================================GPU 光栅化时的裁剪（clip）
GPU 负责「光栅化」（Rasterization），也就是把图层的矢量/位图数据转成最终要显示的像素。

GPU 本来就有「裁剪」的硬件能力：
每个像素在填充之前，GPU 会先检查它是否落在当前可见矩形（clip rect）里。
如果不在范围内，就直接丢掉（discard），不需要额外生成新的 buffer。
这就是我说的 “GPU 可以直接在光栅化时裁剪” ——比如单纯的 masksToBounds = true，等价于告诉 GPU：「绘制时把超出父图层边界的像素直接不要画」。
这种情况 GPU 原生就能做，不需要先把图层渲染到一块 offscreen buffer 再做遮罩。