图片解码有两种方式
1、通过获取绘图上下文，将图片绘制到上下文中，然后从上下文中取出这个图片（优点就是，绘图过程会对图片做优化处理）
//将要解码的图片
UIImage *img = [[UIImage alloc] initWithContentsOfFile:@"..."];
CGImageRef imageRef = img.CGImage;
CGImageAlphaInfo alphaInfo = CGImageGetAlphaInfo(imageRef) & kCGBitmapAlphaInfoMask;
        BOOL hasAlpha = NO;
        if (alphaInfo == kCGImageAlphaPremultipliedLast ||
            alphaInfo == kCGImageAlphaPremultipliedFirst ||
            alphaInfo == kCGImageAlphaLast ||
            alphaInfo == kCGImageAlphaFirst) {
            hasAlpha = YES;
        }
        // BGRA8888 (premultiplied) or BGRX8888
        // same as UIGraphicsBeginImageContext() and -[UIView drawRect:]
        CGBitmapInfo bitmapInfo = kCGBitmapByteOrder32Host;
        bitmapInfo |= hasAlpha ? kCGImageAlphaPremultipliedFirst : kCGImageAlphaNoneSkipFirst;
        CGContextRef context = CGBitmapContextCreate(NULL, width, height, 8, 0, YYCGColorSpaceGetDeviceRGB(), bitmapInfo);
        if (!context) return NULL;
        CGContextDrawImage(context, CGRectMake(0, 0, width, height), imageRef); // 图片解码
        CGImageRef newImage = CGBitmapContextCreateImage(context);
        CFRelease(context);
UIImage *decodedImg = [UIImage imageWithCGImage:newImage]; // 解码后的图片
2、通过CGDataProviderCopyData( CGDataProviderRef cg_nullable provider)进行解码
//将要解码的图片
UIImage *img = [[UIImage alloc] initWithContentsOfFile:@"..."];
CGImageRef imageRef = img.CGImage;
CGColorSpaceRef space = CGImageGetColorSpace(imageRef);
        size_t bitsPerComponent = CGImageGetBitsPerComponent(imageRef);
        size_t bitsPerPixel = CGImageGetBitsPerPixel(imageRef);
        size_t bytesPerRow = CGImageGetBytesPerRow(imageRef);
        CGBitmapInfo bitmapInfo = CGImageGetBitmapInfo(imageRef);
        if (bytesPerRow == 0 || width == 0 || height == 0) return NULL;
        
        CGDataProviderRef dataProvider = CGImageGetDataProvider(imageRef);
        if (!dataProvider) return NULL;
        CFDataRef data = CGDataProviderCopyData(dataProvider); // 对图片解码
        if (!data) return NULL;
        
        CGDataProviderRef newProvider = CGDataProviderCreateWithCFData(data);
        CFRelease(data);
        if (!newProvider) return NULL;
        
        CGImageRef newImage = CGImageCreate(width, height, bitsPerComponent, bitsPerPixel, bytesPerRow, space, bitmapInfo, newProvider, NULL, false, kCGRenderingIntentDefault);
        CFRelease(newProvider);
UIImage *decodedImg = [UIImage imageWithCGImage:newImage]; // 解码后的图片

说明：[UIImage imageNamed:@"name"]得到的图片是已经解码的图片，并且将这个解码好的图片缓存了起来。


那系统是在什么时候解码的呢？
其实显示到屏幕上的图层的绘制是由一个独立与app的渲染引擎操作的，这个引擎只接收解码好的图片，然后直接绘制到屏幕上。
那么图片的解码操作也就是在这个图片将要让渲染引擎绘制的时候进行的，这个操作是在app内部完成的。

如果我们不手动解码，并且图片不是通过[UIImage imageNamed:@"name"]创建的，UIImage是不会对图片进行解码的

==================================================

Surface 在图形系统里是一个比较通用的词，其实可以类比成 “一块显存里用来存放像素数据的区域”。
在 Metal / OpenGL ES 里，Surface 最常见的形式就是 Texture（纹理） 或 Render Target（渲染目标）。
在图形 API（Core Animation / Metal / OpenGL / DirectX）里，Surface 通常指 GPU 可读写的一块像素存储区域。
它可以来自：
纹理（Texture）：用于采样（读取像素）
渲染目标（Render Target / Framebuffer Attachment）：用于绘制输出
显示缓冲区（Swapchain Surface / Layer Backing Store）：最终显示到屏幕的内存区域

CGImage = 一张存在内存里的照片（CPU 知道每个像素的颜色）。
Surface (Texture) = 显卡显存里的一张画布（GPU 能直接访问）。
拷贝过程 = 把照片扫描到显卡的画布上，让 GPU 后续能直接用。


==================================================
CGImage 的存储形式并不固定：
CGImage 其实是一个“像素数据描述符”，它可能：
    指向 已经解码的 bitmap buffer（比如你自己用 CGBitmapContext 生成的图）。
    也可能指向 压缩数据（JPEG/PNG 的解码 provider，只有在真正绘制或访问像素时才会解码）。
CGImage.dataProvider 返回的 CGDataProvider，底层可能是：// CGImage.dataProvider 里取出来的不一定是解码后的像素。
    一个 CFData（解码过的像素）。
    一个解码器对象（延迟解码，需要调用绘制才能触发）。


dataProvider 能告诉你 有数据来源

==================================================


设备上的两种稀缺资源：内存和 CPU。

当你的应用使用更多 CPU 时，会对电池寿命和应用的响应速度产生负面影响
当你的应用以及系统上的其他应用占用更多内存时，也会导致 CPU 使用增加，这会进一步影响电池寿命和性能。
还有比一个大量处理照片内容的应用更适合讨论这个问题的例子吗？比如 Photos 应用。
UIImage 是 UIKit 中用于处理图像数据的高层类。所以，我们这里有一个 UIImage 来表示这张丰富的图像内容。

#####
在经典的 MVC 风格中，UIImage 可以被看作是模型对象，而 UIImageView，是视图对象
UIImage 负责加载图像内容，而 UIImageView 负责显示它，也就是渲染它。

除了渲染是一个连续过程而不是一次性事件之外，还有一个隐藏阶段。理解这个阶段对于衡量应用性能非常重要，这个阶段称为“解码”。
#####

image rendering pipeline:
load->decode->render

缓冲区就是一块连续的内存区域。但当我们讨论由一系列相同大小、通常内部结构也相同的元素组成的内存时，我们会使用“缓冲区”这个术语。
一个非常重要的缓冲区类型是“图像缓冲区”。
图像缓冲区是指用于存储图像的内存表示的缓冲区。缓冲区中的每个元素都描述图像中一个像素的颜色和透明度。因此，这个缓冲区在内存中的大小与它包含的图像大小成正比。

#####
一个特别重要的缓冲区示例叫做“帧缓冲区”（frame buffer）。
帧缓冲区保存的是应用的实际渲染输出。
所以，当你的应用更新视图层级时，UIKit 会将应用的窗口及其所有子视图渲染到帧缓冲区中。
帧缓冲区提供每个像素的颜色信息，显示硬件会读取这些信息以点亮屏幕上的像素。
这个过程是以固定间隔进行的。通常为 60 帧每秒，也就是每 1/60 秒一次。在带有 ProMotion 显示屏的 iPad 上，这个间隔可以快到每 1/120 秒一次。
如果应用中的内容没有变化，显示硬件会从帧缓冲区中读取和之前相同的数据。
但如果你更改了应用中视图的内容，例如，为我们的 image view 分配一个新的 UIImage。
UIKit 会将应用窗口重新渲染到帧缓冲区中。
下一次显示硬件从帧缓冲区读取数据时，就会得到你更新后的内容。
#####

你可以将图像缓冲区与另一种缓冲区进行对比——数据缓冲区，它只是包含一系列字节的缓冲区。
在我们的场景中，我们关注的是包含图像文件的数据缓冲区。可能是从网络下载的，也可能是从磁盘加载的。
包含图像文件的数据缓冲区通常以一些元数据开头，这些元数据描述了缓冲区中存储的图像大小。
然后是图像数据本身，这些数据通常以 JPEG 压缩或 PNG 等形式编码。
#####
这意味着元数据之后的字节并不直接描述图像中像素的颜色信息。
#####

处理流程（pipeline）:
已经标出将由该 image view 渲染填充的帧缓冲区区域。
已经给这个 image view 指定了一个 UIImage。
这个 UIImage 带有一个数据缓冲区，用来表示图像文件的内容。
这可能是从网络下载的，也可能是从磁盘读取的。
但我们需要能够用逐像素的数据来填充帧缓冲区。

#####
数据缓冲区（Data Buffer）：指的是 JPEG、PNG 等编码后的压缩数据，它里面确实包含一些 文件头 / 元数据，比如宽高、颜色空间、bit depth 等信息。
图像缓冲区（Image Buffer）：这是 UIImage 在解码时才分配的内存，存放的是 逐像素解码后的位图数据（raw pixels），比如 RGBA8888，每个像素 4 个字节。
图像缓冲区的大小 确实取决于数据缓冲区头部的图像宽高信息（比如 PNG header 里的 IHDR chunk）。
但它不是直接等于“文件头的某个值”，而是通过“宽 × 高 × 每像素字节数（stride 可能还要按行对齐）”来计算出来的。

举个例子：
一张 PNG 文件，header 中写着 width = 1000, height = 500，颜色深度 8bit，RGBA。
解码后图像缓冲区大小 ≈ 1000 × 500 × 4 = 2,000,000 字节 ≈ 2 MB。
而 PNG 文件本身（数据缓冲区）可能只有 200 KB，因为它是压缩过的。

图像缓冲区大小不是文件里直接存着的一个数字，而是通过文件头里的 宽高 + 像素格式信息 推导出来的。
#####

为了做到这一点，UIImage 会分配一个图像缓冲区，其大小等于数据缓冲区中图像的实际尺寸。
然后执行一个称为“解码”的操作，将 JPEG、PNG 或其他编码格式的图像数据转换为逐像素的图像信息。
接着，根据 image view 的 content mode
当 UIKit 请求图像视图进行渲染时，它会从图像缓冲区复制并缩放图像数据，同时将其写入帧缓冲区。
这个解码阶段可能非常消耗 CPU，尤其是对于大图像而言。
因此，UIImage 会保留这个图像缓冲区，而不是每次 UIKit 请求图像视图渲染时都重新解码，这样解码工作只会执行一次。
因此，对于每个被解码的图像，你的应用可能会在内存中保留一个持久且较大的分配空间。
这个分配的大小与输入图像的尺寸成正比。
而不一定与图像视图在帧缓冲区中实际渲染的尺寸相同。
应用地址空间中这块较大的内存分配可能会挤压其他相关内容，使它们无法被有效引用。
这被称为“内存碎片化”（fragmentation）。
最终，如果你的应用开始占用大量内存，操作系统会介入，并开始对物理内存的内容进行透明压缩。
这个操作需要 CPU 参与，因此除了你应用本身的 CPU 使用之外
你可能会增加你无法控制的全局 CPU 使用率。
最终，你的应用可能占用如此多的物理内存，以至于操作系统需要开始终止进程。
它会从低优先级的后台进程开始。
最终，如果你的应用消耗的内存足够多，你的应用本身也可能被终止。
而且，这些后台进程中有些正在代表用户执行重要任务。
因此，它们可能会在被终止后立即重新启动。
因此，即使你的应用只是在短时间内占用内存，它也可能对 CPU 使用产生长期的连锁影响。
所以，我们希望减少应用使用的内存量。
我们可以通过一种叫做“降采样（downsampling）”的技术来提前应对这一问题。


这样一个事实：我们要显示图像的图像视图实际上比我们要显示的图像本身要小。
通常，Core Animation 框架会负责在渲染阶段将图像缩小，
但通过使用降采样（downsampling）技术，我们可以节省一些内存。
我们要做的是将这个缩小操作捕获到一个称为“缩略图（thumbnail）”的对象中。
这样，我们最终会获得更低的总内存使用量，因为解码后的图像缓冲区更小。
所以，我们先创建一个图像源（image source），然后生成缩略图，再将解码后的图像缓冲区捕获到 UIImage 中。
然后，将这个 UIImage 分配给我们的图像视图。
接着，我们就可以丢弃包含原始图像的数据缓冲区。
这样，我们的应用程序就会拥有一个更小的长期内存占用。

首先，我们将创建一个 CGImageSource 对象。
CGImageSourceCreate 可以接受一个选项字典。
这里我们要传递的重要选项是 ShouldCache 标志。
这告诉 Core Graphics 框架，我们只是创建一个对象来表示存储在该 URL 文件中的信息。
不要立即解码这个图像，只需创建一个表示该信息的对象。
我们将需要从这个 URL 中获取信息。
我们会计算水平方向和垂直方向的尺寸。
基于我们要渲染的缩放比例和点大小，取像素上较大的那个维度。
计算出这些信息后，为我们的缩略图创建一个选项字典。
非常重要的是 CacheImmediately 选项。
传递这个选项时，我们告诉 Core Graphics，当我请求创建缩略图时，就是你应该为我创建解码后的图像缓冲区的时刻。
这样，我们就可以精确控制何时承担解码带来的 CPU 开销。
然后，我们创建缩略图（thumbnail），它是一个 CGImage，并将其返回。
将其封装为 UIImage，并从我们这里写的辅助函数中返回。
为了让大家了解这种技术能节省多少内存，这里我们只是显示一张全屏图像。
这是一张照片，尺寸为 3000 x 2000 像素。
如果不做任何优化，仅在 storyboard 中放置 UIImageView 并分配图像，这个应用程序仅静置不动就会占用 31.5 MB 内存。
使用降采样技术，仅生成与实际显示尺寸相同的图像缓冲区，应用程序的内存使用量可以降到 18.4 MB。
这意味着内存使用量大幅减少。

你可以想象，对于在屏幕小区域显示大量可能很大的图像的应用来说，这有多重要。
例如，“相机胶卷”。
你可能会使用 UICollectionView 来实现这样的视图。
我们使用之前写的辅助函数，将图像降采样到单元格在屏幕上实际显示的尺寸。
也就是说，与其让这些大内存分配一直存在，不如通过这个方法减少内存占用。
不幸的是，这并不能解决另一个常见问题，这种问题常出现在可滚动视图中，比如 table view 和 collection view。
你可能以前见过这个问题：当你在应用中滚动时，界面开始出现卡顿。