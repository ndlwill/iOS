https://developer.apple.com/documentation/audiotoolbox

iOS2.0
Record or play audio, convert formats, parse audio streams, and configure your audio session.

The AudioToolbox framework provides interfaces for recording, playback, and stream parsing. 
In iOS, the framework provides additional interfaces for managing audio sessions.


Spatial audio（空间音频）
指的是一种音频技术，通过模拟或重现真实世界中声音在空间中的位置和方向感，使得听众可以感知到声音的位置、距离和方向。传统的立体声音频通常只包含左右声道，而空间音频则可以提供更加逼真的听觉体验，使得声音似乎来自于不同的方向和位置。

A multichannel audio stream（多通道音频流）
指的是包含多个音频通道的音频流。在多通道音频流中，每个通道都可以包含不同的音频信号，例如左前声道、右前声道、中央声道、低音炮声道等。多通道音频流常用于环绕声系统或多声道音频录制中，以提供更加丰富和立体的听觉体验。

PCM（脉冲编码调制）是用于音频的一种数字编码格式，而不是用于视频的。
PCM 是一种原始的数字音频编码格式，它将模拟音频信号转换为数字信号，以便在数字系统中进行处理和传输。
在 PCM 格式中，音频信号被采样并量化为数字样本，然后用一系列数字值来表示音频波形的振幅。
这些数字样本按照时间顺序排列，形成了音频流。PCM 格式不对音频进行压缩，因此可以保留音频的原始质量和精度。

User data（用户数据）： 用户数据是指音频文件中由用户自定义的任何附加信息或元数据。
这些数据通常包含了关于音频文件的描述、标签、作者、专辑信息、歌词、封面图像等内容。
用户数据的格式和内容可以根据具体的应用需求和文件格式进行自定义，例如在 MP3 文件中，用户数据通常存储在 ID3 标签中。

Global information（全局信息）： 全局信息是指音频文件中与整个文件相关的共享信息或元数据。
这些信息通常包括了音频的基本参数和属性，如采样率、位深度、声道数、时长等。全局信息提供了对整个音频文件的整体描述，用于指导音频文件的解码和播放过程。

Audio Units（音频单元）
音频单元是 iOS 中用于音频处理和效果的模块化单元。它们可以实现各种音频处理功能，如回声消除、混响、均衡器等，以及实时音频效果，如变声、合唱等。开发人员可以使用 Core Audio 框架来创建、加载和配置音频单元，以实现定制的音频处理流程。

Audio Codecs（音频编解码器）
音频编解码器用于将音频数据进行压缩和解压缩，以减小文件大小或者提高网络传输效率。iOS 支持多种常见的音频编解码器，如 AAC、MP3、Opus 等，开发人员可以使用系统提供的编解码器来对音频数据进行编码和解码操作。

Core Audio 框架
Core Audio 是 iOS 中用于音频处理的核心框架，提供了丰富的音频处理功能和 API，包括音频单元、音频会话管理、音频图表等。开发人员可以使用 Core Audio 框架来查找、加载和配置音频组件，实现各种音频处理和效果。

==================================================Audio Units
Audio Components:
Find, load, and configure audio components, such as Audio Units and audio codecs.

Use the Audio Components API to register and discover audio units, codecs, and other loadable code modules. 
This API replaces the Component Manager API used prior to macOS 10.6. The system searches for loadable bundles with a .audiocomp or .component filename extension in the following locations:
~/Library/Audio/Plug-Ins/Components
/Library/Audio/Plug-Ins/Components
/System/Library/Components
The bundle Info.plist file needs to contain an AudioComponents item whose value is an array of dictionaries.



Audio Unit Properties:
Obtain information about the built-in mixers, equalizers, filters, effects, and other Audio Unit app extensions.


==================================================Playback and Recording
Audio Queue Services:
Connect to audio hardware and manage the recording or playback process.

a C programming interface in the Audio Toolbox framework, which is part of Core Audio.
An audio queue is a software object you use for recording or playing audio. An audio queue does the work of:
Connecting to audio hardware
Managing memory
Employing codecs, as needed, for compressed audio formats
Mediating playback or recording

Audio Queue Services enables you to record and play audio in linear PCM, in compressed formats (such as Apple Lossless and AAC), and in other formats for which users have installed codecs. 
Audio Queue Services also supports scheduled playback and synchronization of multiple audio queues and synchronization of audio with video.



Audio Services:
Play short sounds or trigger a vibration effect on iOS devices with the appropriate hardware.

System Sound Services provides a C interface for playing short sounds and for invoking vibration on iOS devices that support vibration.
You can use System Sound Services to play short (30 seconds or shorter) sounds. 
The interface does not provide level, positioning, looping, or timing control, and does not support simultaneous playback: You can play only one sound at a time. 
You can use System Sound Services to provide audible alerts. On some iOS devices, alerts can include vibration.



Music Player:
Create and play a sequence of tracks, and manage aspects of playback in response to standard events.


==================================================Audio Files and Formats
Audio Format Services:
Access information about audio formats and codecs.
a C interface for obtaining information about audio formats and codecs.
音频格式是指存储音频数据的文件格式，而编解码器则是用于将音频数据编码（压缩）和解码（解压）的算法或程序。
MP3： MP3（MPEG-1 Audio Layer III）是一种流行的音频格式，其编解码器使用有损压缩算法来减小文件大小。常见的 MP3 编解码器包括 LAME、Fraunhofer 等。
AAC： AAC（Advanced Audio Coding）是一种高级音频编码格式，常用于音乐和流媒体传输。常见的 AAC 编解码器包括 Apple AAC、Fraunhofer AAC 等。
WAV： WAV（Waveform Audio File Format）是一种无损音频格式，通常用于存储原始的 PCM 数据。WAV 文件通常不经过压缩，因此文件大小较大。
FLAC： FLAC（Free Lossless Audio Codec）是一种开源的无损音频编码格式，可实现无损压缩，以减小文件大小而不损失音频质量。
OGG： OGG 是一种开源的音频容器格式，通常使用 Vorbis 编解码器来进行有损压缩。OGG 文件常用于音乐和音频流传输。
PCM： PCM（Pulse Code Modulation）是一种原始的数字音频格式，将模拟音频信号转换为数字信号，通常不经过压缩。PCM 数据可以以不同的采样率和位深度进行存储。
Opus： Opus 是一种开放、免费的音频编码格式，通常用于实时通信和网络流媒体传输。Opus 提供了高质量的音频编解码器，支持多种比特率和频率范围。



Audio File Services:
Read or write a variety of audio data to or from disk or a memory buffer.
a C programming interface that enables you to read or write a wide variety of audio data to or from disk or a memory buffer.
With Audio File Services you can:
Create, initialize, open, and close audio files
Read and write audio files
Optimize audio files
Work with user data and global information



Extended Audio File Services:
Read and write compressed files and linear PCM audio files using a simplified interface.
Extended Audio File Services provides simplified audio file access, combining features of Audio File Services and Audio Converter Services. 
It provides a unified interface for reading and writing compressed as well as linear PCM audio files.



Audio File Stream Services:
Parse streamed audio files as the data arrives on the user’s computer.
Audio File Stream Services provides the interface for parsing streamed audio files—in which only a limited window of data is available at a time.

Audio file streams, by nature, are not random access. 
When you request data from a stream, earlier data might no longer be accessible and later data might not yet be available. 
In addition, the data you obtain (and then provide to a parser) might include partial packets. 
To parse streamed audio data, then, a parser must remember data from partially satisfied requests, and must be able to wait for the remainder of that data. 
In other words, a parser must be able to suspend parsing as needed and then resume where it left off.

To use a parser, you pass data from a streamed audio file, as you acquire it, to the parser. 
When the parser has a complete packet of audio data or a complete property, it invokes a callback function. 
Your callbacks then process the parsed data—such as by playing it or writing it to disk.

Here, in outline form, is a typical usage pattern for an audio file stream parser:
1. Create a new audio file stream parser by calling the AudioFileStreamOpen(_:_:_:_:_:) function. 
Pass pointers to your callback functions for audio data and metadata (AudioFileStream_PacketsProc and AudioFileStream_PropertyListenerProc). 
The AudioFileStreamOpen(_:_:_:_:_:) function gives you a reference to the new parser.
2. Acquire some streamed data. Call the AudioFileStreamParseBytes(_:_:_:_:) function when you have data to pass to the parser. 
Send the data to the parser sequentially and, if possible, without gaps.
（1）
When the parser acquires a usable buffer of audio data, it invokes your audio data callback. Your callback can then play the data, write it to a file, or otherwise process it.
（2）
When the parser acquires metadata, it invokes your property callback—which in turn can obtain the property value by calling the AudioFileStreamGetPropertyInfo(_:_:_:_:) and AudioFileStreamGetProperty(_:_:_:_:) functions.
3. When finished parsing a stream, call the AudioFileStreamClose(_:) function to close and deallocate the parser.

Audio File Stream Services supports the following audio data types:
AIFF
AIFC
WAVE
CAF
NeXT
ADTS
MPEG Audio Layer 3
AAC



Audio File Components:
Get information about audio file formats, and about files containing audio data.



Core Audio File Format:
Parse the structure of Core Audio files.



==================================================Utilities
Analyzing audio performance with Instruments:
Ensure a smooth and immersive audio experience in your apps using Audio System Trace.
https://developer.apple.com/documentation/audiotoolbox/analyzing_audio_performance_with_instruments



Audio Converter Services:
Convert between linear PCM audio formats, and between linear PCM and compressed formats.
Audio converter objects convert between various linear PCM audio formats. They can also convert between linear PCM and compressed formats. Supported transformations include the following:
PCM bit depth
PCM sample rate
PCM floating point to and from PCM integer
PCM interleaved to and from PCM deinterleaved
PCM to and from compressed formats

A single audio converter may perform more than one of the listed transformations.



Audio Session Support:
Describe the properties that you associate with audio sessions and audio routes.
Important:
The AudioSession API has been completely deprecated in iOS 7.0. See AVAudioSession for the Objective-C implementation of these functions.

Audio Session Services lets you specify the intended audio behavior for your iOS app. 
For example, you can specify whether you intend for your app’s audio to silence other apps or to mix with their audio. 
You also use this API to specify your app’s behavior when it is interrupted, such as by a phone call. 
When the system knows your intentions, it configures the audio hardware in the device to satisfy those intentions, as possible.
These functions apply only to iOS. They do not apply to macOS.



Audio Toolbox Debugging:
Obtain the internal state of Core Audio objects during the development and debugging of your code.
The AudioToolbox.h header file provides auxiliary functions for obtaining the internal state of a Core Audio object. Use these functions during development and debugging.



Workgroup Management:
Coordinate the activity of custom real-time audio threads with those of the system and other processes.
Real-time audio rendering often requires coordination between the threads of an app, the system, and the threads of any active Audio Unit plug-ins. Workgroups provide the mechanism to coordinate the efforts of these different processes, and ensure that they execute on the same schedule. In an Audio Unit, use a render context observer to retrieve the workgroup that the host app uses for real-time audio rendering. 
In an app, fetch the workgroup for a Core Audio device directly from the device or from your AUAudioUnit object.

If your app has real-time rendering threads that operate on their own deadlines, create your own workgroup using the AudioWorkIntervalCreate(_:_:_:) function. 
Use your custom workgroup to set and update the rendering schedule for your threads.

###Understanding Audio Workgroups###
Learn how to optimize real-time rendering performance with the Audio Workgroups API.
The Audio Workgroups API is a new feature available in macOS 11, iOS 14.0, watchOS 7.0 and tvOS 14.0.



Audio Codec:
Translate audio data from one format to another.



Clock Utilities:
Manage time-related information associated with audio playback.